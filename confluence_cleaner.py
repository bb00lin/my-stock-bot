import os
import requests
import json
import re
import sys
from datetime import datetime
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup, Tag

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
KEEP_LIMIT = 5  # ä¿ç•™æœ€æ–°çš„å¹¾ç­†è³‡æ–™

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

parsed = urlparse(RAW_URL)
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

def find_latest_report():
    """æ‰¾åˆ°æœ€æ–°çš„é€±å ± (View æ ¼å¼)"""
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    url = f"{API_ENDPOINT}/search"
    params = {'cql': cql, 'limit': 1, 'expand': 'body.view'}
    
    response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    response.raise_for_status()
    results = response.json().get('results', [])
    if not results:
        print("âš ï¸ æ‰¾ä¸åˆ°é€±å ±")
        sys.exit(1)
    return results[0]

def extract_first_project_link(report_body):
    """å¾ HTML ä¸­æŠ“å– Project æ¬„ä½çš„ç¬¬ä¸€å€‹é€£çµ (å¼·åŠ›è§£æç‰ˆ)"""
    soup = BeautifulSoup(report_body, 'html.parser')
    
    tables = soup.find_all('table')
    for table in tables:
        headers = []
        header_row = table.find('tr')
        if not header_row: continue
        
        for cell in header_row.find_all(['th', 'td']):
            headers.append(cell.get_text().strip())
            
        if "Project" in headers:
            proj_idx = headers.index("Project")
            rows = table.find_all('tr')
            
            # æ‰¾ç¬¬ä¸€åˆ—æœ‰è³‡æ–™çš„ row
            for row in rows[1:]:
                cols = row.find_all('td')
                if len(cols) > proj_idx:
                    link_tag = cols[proj_idx].find('a')
                    
                    if link_tag:
                        # æ–¹æ³• 1: å˜—è©¦æŠ“ data-linked-resource-id (æœ€æº–)
                        page_id = link_tag.get('data-linked-resource-id')
                        if page_id:
                            print(f"ğŸ¯ é–å®šç›®æ¨™ (é€é data-id): {page_id}")
                            return {'id': page_id}
                        
                        # æ–¹æ³• 2: åˆ†æ href ç¶²å€
                        href = link_tag.get('href', '')
                        print(f"   â„¹ï¸ åˆ†æé€£çµ: {href}")
                        
                        # æƒ…æ³ A: ...?pageId=12345
                        if 'pageId=' in href:
                            parsed_url = urlparse(href)
                            qs = parse_qs(parsed_url.query)
                            if 'pageId' in qs:
                                page_id = qs['pageId'][0]
                                print(f"ğŸ¯ é–å®šç›®æ¨™ (é€é href pageId): {page_id}")
                                return {'id': page_id}
                        
                        # æƒ…æ³ B: /pages/12345/Title
                        match = re.search(r'/pages/(\d+)/', href)
                        if match:
                            page_id = match.group(1)
                            print(f"ğŸ¯ é–å®šç›®æ¨™ (é€é href path): {page_id}")
                            return {'id': page_id}

                        # æ–¹æ³• 3: å¦‚æœçœŸçš„éƒ½æ²’æœ‰ IDï¼Œåªå¥½æŠ“æ–‡å­— (ä½†é€™æ¬¡æˆ‘å€‘çŸ¥é“é€™å¯èƒ½ä¸æº–)
                        title = link_tag.get_text().strip()
                        print(f"âš ï¸ è­¦å‘Šï¼šç„¡æ³•å¾é€£çµè§£æ IDï¼Œå˜—è©¦ä½¿ç”¨æ–‡å­—æ¨™é¡Œ: {title}")
                        # é€™è£¡æˆ‘å€‘åšä¸€å€‹å¤§è†½çš„çŒœæ¸¬ï¼šå¦‚æœæ–‡å­—æ˜¯ 'AhGW'ï¼Œé€šå¸¸æ¨™é¡Œæ˜¯ 'WeeklyStatus_AhGW'
                        # ä½†ç‚ºäº†ä¿éšªï¼Œæˆ‘å€‘å…ˆå›å‚³æ–‡å­—ï¼Œè®“å¾Œé¢ try error
                        return {'title': title}

    print("âš ï¸ æ‰¾ä¸åˆ° Project é€£çµ")
    return None

def get_page_by_id(page_id):
    """é€é ID å–å¾—é é¢è³‡è¨Š"""
    url = f"{API_ENDPOINT}/{page_id}"
    params = {'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    if resp.status_code == 200:
        return resp.json()
    print(f"âŒ é€é ID {page_id} æ‰¾ä¸åˆ°é é¢")
    return None

def get_page_by_title(title):
    """é€éæ¨™é¡Œå–å¾—é é¢è³‡è¨Š"""
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    results = resp.json().get('results', [])
    if results:
        return results[0]
    
    # è‡ªå‹•å˜—è©¦è£œä¸Š WeeklyStatus_ å‰ç¶´ (é‡å°æ‚¨çš„å‘½åç¿’æ…£åšçš„è£œæ•‘)
    if not title.startswith("WeeklyStatus_"):
        alt_title = f"WeeklyStatus_{title}"
        print(f"   å˜—è©¦çŒœæ¸¬æ¨™é¡Œ: {alt_title}")
        params['title'] = alt_title
        resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        results = resp.json().get('results', [])
        if results:
            print(f"   âœ… çŒœæ¸¬æˆåŠŸï¼")
            return results[0]

    return None

def is_red_row(tr):
    """åˆ¤æ–·ç´…å­—"""
    tags_with_style = tr.find_all(lambda tag: tag.has_attr('style'))
    for tag in tags_with_style:
        style = tag['style'].lower()
        if 'rgb(255, 0, 0)' in style or '#ff0000' in style:
            return True
    if tr.find('font', color="red") or tr.find('font', color="#ff0000"):
        return True
    return False

def clean_project_page_content(html_content):
    """æ ¸å¿ƒé‚è¼¯ï¼šç˜¦èº« + æ­¸æª”"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    history_header = soup.find(lambda tag: tag.name in ['h1', 'h2'] and 'History' in tag.get_text())
    
    if not history_header:
        print("   â„¹ï¸ æ‰¾ä¸åˆ° History å€å¡Šï¼Œæ­£åœ¨å»ºç«‹...")
        history_header = soup.new_tag('h1')
        history_header.string = "History"
        soup.append(history_header)
    
    all_headers = soup.find_all(['h3', 'h4']) 
    changed = False
    
    for header in all_headers:
        # ç°¡å–®åˆ¤å®šï¼šå¦‚æœåœ¨ History ä¹‹å¾Œå°±ä¸è™•ç†
        if history_header and header.sourceline and history_header.sourceline:
             if header.sourceline > history_header.sourceline: continue
            
        header_text = header.get_text().strip()
        if header_text.lower() in ['history', 'work item table']: continue
            
        next_node = header.find_next_sibling()
        target_table = None
        while next_node:
            if next_node.name == 'table':
                target_table = next_node
                break
            if next_node.name in ['h1', 'h2', 'h3', 'h4']: break
            next_node = next_node.find_next_sibling()
            
        if not target_table: continue
            
        print(f"   ğŸ” æª¢æŸ¥é …ç›®: {header_text}")
        
        tbody = target_table.find('tbody')
        if not tbody: continue
        rows = tbody.find_all('tr')
        if not rows: continue
        
        data_rows = rows[1:] 
        keep_rows = []
        archive_rows = []
        
        count = 0
        for row in data_rows:
            if is_red_row(row):
                keep_rows.append(row)
                print("      ğŸ”´ ç™¼ç¾ç´…å­—ï¼Œå¼·åˆ¶ä¿ç•™")
                continue
            
            if count < KEEP_LIMIT:
                keep_rows.append(row)
                count += 1
            else:
                archive_rows.append(row)
        
        if archive_rows:
            print(f"      âœ‚ï¸ éœ€æ­¸æª” {len(archive_rows)} ç­†è³‡æ–™...")
            changed = True
            
            for row in archive_rows:
                row.extract()
                
            # æ”¾å…¥ History
            hist_item_header = None
            curr = history_header.next_sibling
            while curr:
                if curr.name in ['h3', 'h4'] and curr.get_text().strip() == header_text:
                    hist_item_header = curr
                    break
                curr = curr.next_sibling
            
            hist_table = None
            if hist_item_header:
                curr = hist_item_header.next_sibling
                while curr:
                    if curr.name == 'table':
                        hist_table = curr
                        break
                    if curr.name in ['h1', 'h2', 'h3', 'h4']: break
                    curr = curr.next_sibling
            else:
                print(f"      ğŸ†• History ä¸­ç„¡ [{header_text}]ï¼Œæ­£åœ¨æ–°å»º...")
                new_h4 = soup.new_tag(header.name)
                new_h4.string = header_text
                soup.append(new_h4)
                
                hist_table = soup.new_tag('table')
                orig_thead = rows[0]
                import copy
                new_thead = copy.copy(orig_thead) 
                hist_table.append(new_thead)
                soup.append(hist_table)
            
            if not hist_table.find('tbody'):
                hist_table.append(soup.new_tag('tbody'))
                
            for row in archive_rows:
                hist_table.append(row)
                
    return str(soup) if changed else None

def update_page(page_data, new_content):
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜é é¢: {page_data['title']} (éœé»˜æ¨¡å¼)...")
    url = f"{API_ENDPOINT}/{page_data['id']}"
    payload = {
        "version": {"number": page_data['version']['number'] + 1, "minorEdit": True},
        "title": page_data['title'],
        "type": "page",
        "body": {
            "storage": {
                "value": new_content,
                "representation": "storage"
            }
        }
    }
    resp = requests.put(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), 
                       headers=get_headers(), data=json.dumps(payload))
    resp.raise_for_status()
    print("âœ… æ›´æ–°æˆåŠŸï¼")

def main():
    print("=== Confluence å°ˆæ¡ˆé é¢æ•´ç†æ©Ÿå™¨äºº (Test Mode: 1st Link) ===")
    
    report = find_latest_report()
    target_info = extract_first_project_link(report['body']['view']['value'])
    
    if not target_info:
        print("çµæŸï¼šæ²’æœ‰æ‰¾åˆ°å¯è™•ç†çš„å°ˆæ¡ˆé€£çµã€‚")
        return

    # é€™è£¡åšäº†é›™é‡ä¿éšªï¼šæœ‰ ID ç”¨ IDï¼Œæ²’ ID ç”¨æ¨™é¡ŒçŒœ
    if 'id' in target_info:
        page_data = get_page_by_id(target_info['id'])
    else:
        page_data = get_page_by_title(target_info['title'])
        
    if not page_data:
        print(f"âŒ æœ€çµ‚å¤±æ•—ï¼šç„¡æ³•æ‰¾åˆ°å°æ‡‰é é¢")
        return
        
    print(f"ğŸ“– è®€å–é é¢: {page_data['title']} (ID: {page_data['id']})")
    
    new_content = clean_project_page_content(page_data['body']['storage']['value'])
    
    if new_content:
        update_page(page_data, new_content)
    else:
        print("ğŸ‘Œ é é¢ç„¡éœ€è®Šæ›´")

if __name__ == "__main__":
    main()
