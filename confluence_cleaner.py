import os
import requests
import json
import re
import sys
import copy
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse, parse_qs, unquote
from bs4 import BeautifulSoup, Tag, NavigableString

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
MASTER_PAGE_ID = os.environ.get("MASTER_PAGE_ID")
KEEP_LIMIT = 5 

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

parsed = urlparse(RAW_URL)
HOST_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{HOST_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

# --- 1. æœå°‹é€±å ± ---
def find_latest_report():
    if MASTER_PAGE_ID:
        print(f"ğŸ¯ åµæ¸¬åˆ° MASTER_PAGE_ID ({MASTER_PAGE_ID})")
        url = f"{API_ENDPOINT}/{MASTER_PAGE_ID}"
        params = {'expand': 'body.view,body.storage,version'}
        try:
            r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
            r.raise_for_status()
            return r.json()
        except Exception as e:
            print(f"âŒ è®€å–å¤±æ•—: {e}")
            sys.exit(1)

    print("ğŸ” æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    url = f"{API_ENDPOINT}/search"
    params = {'cql': cql, 'limit': 1, 'expand': 'body.view,body.storage,version'}
    r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    r.raise_for_status()
    results = r.json().get('results', [])
    if not results:
        print("âš ï¸ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°é€±å ±")
        sys.exit(1)
    print(f"âœ… æœå°‹æˆåŠŸ: {results[0]['title']}")
    return results[0]

def resolve_real_page_id(href_link):
    if not href_link: return None
    if href_link.startswith('/'): full_url = f"{HOST_URL}{href_link}"
    else: full_url = href_link
    if 'pageId=' in full_url:
        qs = parse_qs(urlparse(full_url).query)
        if 'pageId' in qs: return qs['pageId'][0]
    m = re.search(r'/pages/(\d+)', full_url)
    if m: return m.group(1)
    try:
        r = requests.head(full_url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), allow_redirects=True, timeout=10)
        final_url = r.url
        qs = parse_qs(urlparse(final_url).query)
        if 'pageId' in qs: return qs['pageId'][0]
        m = re.search(r'/pages/(\d+)', final_url)
        if m: return m.group(1)
    except: pass
    return None

def extract_all_project_links(report_body):
    soup = BeautifulSoup(report_body, 'lxml')
    tables = soup.find_all('table')
    project_targets = []
    for table in tables:
        h_row = table.find('tr')
        if not h_row: continue
        headers = [c.get_text().strip() for c in h_row.find_all(['th', 'td'])]
        if "Project" in headers:
            print("âœ… æ‰¾åˆ° Project Status è¡¨æ ¼")
            proj_idx = headers.index("Project")
            for row in table.find_all('tr')[1:]:
                cols = row.find_all('td')
                if len(cols) > proj_idx:
                    for link in cols[proj_idx].find_all('a'):
                        target = {'name': link.get_text().strip()}
                        pid = link.get('data-linked-resource-id')
                        if pid: target['id'] = pid
                        else:
                            real_id = resolve_real_page_id(link.get('href', ''))
                            if real_id: target['id'] = real_id
                            else: target['title'] = target['name']
                        if target.get('id') or target.get('title'):
                            exists = False
                            for t in project_targets:
                                if t.get('id') and t['id'] == target.get('id'): exists = True
                            if not exists: project_targets.append(target)
            break 
    return project_targets

def get_page_by_id(page_id):
    url = f"{API_ENDPOINT}/{page_id}"
    params = {'expand': 'body.storage,version'}
    r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    return r.json() if r.status_code == 200 else None

def get_page_by_title(title):
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version'}
    r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    res = r.json().get('results', [])
    if res: return res[0]
    if not title.startswith("WeeklyStatus_"):
        print(f"   å˜—è©¦è£œå…¨æ¨™é¡Œ: WeeklyStatus_{title}")
        r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params={'title': f"WeeklyStatus_{title}", 'expand': 'body.storage,version'})
        res = r.json().get('results', [])
        if res: return res[0]
    return None

# --- V38: è¤‡è£½èˆ‡ä¿®å‰ªé‚è¼¯ ---

def is_date_header(text):
    if not text: return False
    return bool(re.search(r'\[\d{4}/\d{1,2}/\d{1,2}\]', text[:50]))

# æª¢æŸ¥ç¯€é»æœ¬èº«æ˜¯å¦å¸¶æœ‰ç´…è‰²å±¬æ€§ (ç²¾ç¢ºå®šç¾©)
def is_style_red(tag):
    if not isinstance(tag, Tag): return False
    red_patterns = [
        r'color:\s*red', r'#ff0000', r'#de350b', r'#bf2600', r'#ff5630', r'#ce0000', 
        r'#c9372c', r'#C9372C', 
        r'rgb\(\s*255', r'rgb\(\s*222', r'rgb\(\s*201', r'rgb\(\s*191', 
        r'--ds-text-danger', r'--ds-icon-accent-red'
    ]
    combined_regex = re.compile('|'.join(red_patterns), re.IGNORECASE)
    
    if tag.has_attr('style') and combined_regex.search(tag['style']): return True
    if tag.name == 'font' and tag.has_attr('color') and combined_regex.search(tag['color']): return True
    return False

# æª¢æŸ¥ä¸€å€‹æ–‡å­—ç¯€é»çš„çˆ¶å±¤éˆä¸­æ˜¯å¦æœ‰ç´…è‰²æ¨£å¼
def is_context_red(node):
    curr = node.parent
    while curr and curr.name not in ['td', 'body', 'html']:
        if is_style_red(curr): return True
        curr = curr.parent
    return False

# ã€V38 æ ¸å¿ƒã€‘ï¼šä¿®å‰ªæ¨¹ (Prune Tree)
# ç›´æ¥åœ¨å‚³å…¥çš„ soup ç‰©ä»¶ä¸Šé€²è¡Œä¿®æ”¹ï¼Œç§»é™¤é»‘å­—
def prune_non_red_content(soup_fragment):
    # 1. æ‰¾å‡ºæ‰€æœ‰æ–‡å­—ç¯€é» (Leaf Nodes)
    # æˆ‘å€‘ä½¿ç”¨ list() å¼·åˆ¶å–å‡ºæ‰€æœ‰ç¯€é»ï¼Œé¿å…åœ¨éæ­·æ™‚ä¿®æ”¹çµæ§‹å°è‡´è·³é
    text_nodes = [t for t in soup_fragment.find_all(string=True)]
    
    for text_node in text_nodes:
        if not text_node.strip(): continue # å¿½ç•¥ç©ºç™½æ’ç‰ˆ
        
        # åˆ¤æ–·ä¿ç•™æ¢ä»¶
        is_date = is_date_header(str(text_node))
        is_red = is_context_red(text_node)
        
        # å¦‚æœä¸æ˜¯æ—¥æœŸï¼Œä¸”ä¸æ˜¯ç´…è‰² -> å®ƒæ˜¯é»‘å­— -> åˆªé™¤
        if not is_date and not is_red:
            text_node.extract()

    # 2. æ¸…ç†ç©ºå®¹å™¨ (Empty Containers)
    # æ–‡å­—åˆªé™¤å¾Œï¼Œå¯èƒ½æœƒå‰©ä¸‹ç©ºçš„ <p></p> æˆ– <li></li>ï¼Œéœ€è¦ç§»é™¤
    # é‡è¤‡åŸ·è¡Œç›´åˆ°æ²’æœ‰ç©ºå®¹å™¨ç‚ºæ­¢ (å› ç‚ºåˆªé™¤å­ç¯€é»å¯èƒ½å°è‡´çˆ¶ç¯€é»è®Šç©º)
    while True:
        # å°‹æ‰¾ç©ºæ¨™ç±¤ (æ²’æœ‰æ–‡å­—å…§å®¹ä¸”æ²’æœ‰åœ–ç‰‡ç­‰å…¶ä»–è³‡æº)
        # æ³¨æ„ï¼š<br> æ›è¡Œç¬¦è™Ÿå¦‚æœä¸è¢«ä¿ç•™ï¼Œæ’ç‰ˆæœƒäº‚ï¼Œæ‰€ä»¥è¦å°å¿ƒ
        # é€™è£¡ç­–ç•¥ï¼šå¦‚æœæ¨™ç±¤å…§æ²’æœ‰ä»»ä½•å¯è¦‹æ–‡å­—ï¼Œå°±åˆªé™¤
        
        # æ‰¾å‡ºæ‰€æœ‰æ¨™ç±¤ï¼Œç”±æ·±åˆ°æ·º
        tags = soup_fragment.find_all(True)
        removed_count = 0
        
        for tag in tags:
            # è·³é <br>, <img> ç­‰ç©ºå…ƒç´ 
            if tag.name in ['br', 'img', 'hr']: continue
            
            # æª¢æŸ¥æ˜¯å¦é‚„æœ‰å…§å®¹
            if not tag.get_text(strip=True):
                # ç¢ºå¯¦ç©ºäº†ï¼Œåˆªé™¤
                tag.extract()
                removed_count += 1
        
        if removed_count == 0: break

    return soup_fragment

def split_cell_content(cell_soup):
    entries = []
    current_entry = []
    
    # é€™è£¡çš„é‚è¼¯è¦ç¨å¾®æ”¾å¯¬ï¼Œå› ç‚ºæˆ‘å€‘ç¾åœ¨æ˜¯æ•´å¡Šè¤‡è£½ï¼Œ
    # split ä¸»è¦åªæ˜¯ç‚ºäº†é…åˆæ—¢æœ‰çš„ç¨‹å¼æ¶æ§‹è¨ˆç®— KEEP_LIMITã€‚
    # ç‚ºäº†ä¿æŒæ ¼å¼ï¼Œæˆ‘å€‘å…¶å¯¦ä¸éœ€è¦çœŸçš„ split ä¸¦é‡çµ„ï¼Œ
    # è€Œæ˜¯æ‡‰è©²æŠŠæ•´å€‹ Cell è¤‡è£½ä¸‹ä¾†ï¼Œç„¶å¾Œä¿®å‰ªã€‚
    
    # ä½†æ˜¯ï¼Œä½¿ç”¨è€…çš„éœ€æ±‚æ˜¯ "åªå–å‰ 5 å€‹é …ç›®"ã€‚
    # æ‰€ä»¥æˆ‘å€‘é‚„æ˜¯å¾—è¾¨è­˜å‡º "é …ç›®"ã€‚
    
    # ç°¡å–®èµ·è¦‹ï¼ŒV38 ç­–ç•¥ï¼š
    # 1. è¤‡è£½æ•´å€‹ Cell å…§å®¹ã€‚
    # 2. å°è¤‡è£½å“é€²è¡Œã€Œä¿®å‰ªé»‘å­—ã€ã€‚
    # 3. ä¿®å‰ªå®Œå¾Œï¼Œå…§å®¹å·²ç¶“æ˜¯ä¹¾æ·¨çš„ç´…å­—äº†ã€‚
    # 4. ç›´æ¥æŠŠé€™å€‹ä¹¾æ·¨çš„å…§å®¹ç•¶ä½œä¸€å€‹ "å¤§é …ç›®" å›å‚³å³å¯ã€‚
    # 5. é€™æ¨£å¯ä»¥å®Œç¾ä¿ç•™åŸæœ¬çš„æ’ç‰ˆã€‚
    
    return [cell_soup] # å½è£æˆä¸€å€‹é …ç›®ï¼Œç”±å¤–éƒ¨è™•ç†

def clean_project_page_content(html_content, page_title):
    soup = BeautifulSoup(html_content, 'lxml')
    extracted_summary_items = []
    
    main_table = None
    all_tables = soup.find_all('table')
    for t in all_tables:
        if t.find_parent('ac:structured-macro'): continue
        headers = [c.get_text().strip() for c in t.find_all('th')]
        if "Item" in headers and "Update" in headers: main_table = t; break
    if not main_table:
        print(f"   âš ï¸  [{page_title}] æ‰¾ä¸åˆ°ä¸»è¡¨æ ¼ï¼Œè·³éã€‚")
        return None, []

    print(f"   ğŸ” [{page_title}] æ‰¾åˆ°ä¸»è¡¨æ ¼ï¼Œé–‹å§‹è¤‡è£½èˆ‡ä¿®å‰ª...")
    sys.stdout.flush()
    rows = main_table.find_all('tr', recursive=False)
    if not rows and main_table.find('tbody', recursive=False):
        rows = main_table.find('tbody', recursive=False).find_all('tr', recursive=False)
    if not rows: return None, []

    header_row = rows[0]
    headers = [c.get_text().strip() for c in header_row.find_all(['th', 'td'], recursive=False)]
    try: item_idx = headers.index("Item"); update_idx = headers.index("Update")
    except ValueError: return None, []

    total_rows = len(rows) - 1
    
    for i, row in enumerate(rows[1:]):
        sys.stdout.write(f"\r      Scanning Row {i+1}/{total_rows} ...")
        sys.stdout.flush()
        cols = row.find_all('td', recursive=False)
        if len(cols) <= max(item_idx, update_idx): continue
        
        update_cell = cols[update_idx]
        if update_cell.find('table'): continue

        # ã€V38 æ ¸å¿ƒé‚è¼¯ã€‘
        # 1. æ·±å±¤è¤‡è£½æ•´å€‹ Cell (ä¿ç•™æ‰€æœ‰æ ¼å¼ï¼šul, li, strong, style...)
        cell_clone = copy.copy(update_cell) # copy Tag æœƒé€£åŒå­æ¨¹ä¸€èµ·è¤‡è£½
        
        # 2. åŸ·è¡Œä¿®å‰ªï¼šåˆªé™¤æ‰€æœ‰é»‘å­—
        pruned_content = prune_non_red_content(cell_clone)
        
        # 3. æª¢æŸ¥ä¿®å‰ªå¾Œæ˜¯å¦é‚„æœ‰å¯¦è³ªå…§å®¹
        if pruned_content.get_text(strip=True):
            # é€™è£¡æˆ‘å€‘æŠŠä¿®å‰ªå¾Œçš„å…§å®¹åŒ…è£æˆä¸€å€‹ list å‚³å‡ºå»
            # ç‚ºäº†é…åˆ update_main_report_summary çš„ä»‹é¢ (å®ƒé æœŸä¸€çµ„ nodes)
            extracted_summary_items.append(list(pruned_content.contents))

    print(f"\r      Scanning Row {total_rows}/{total_rows} (Done)        ")
    if extracted_summary_items:
        print(f"      ğŸ“Œ æœ¬å°ˆæ¡ˆæ¡é›†åˆ°ç´…å­—æ‘˜è¦ (æ ¼å¼ä¿ç•™)")
    
    return None, extracted_summary_items # Read-Only

def update_page(page_data, new_content):
    pass

def update_main_report_summary(main_report_data, summary_data):
    if not summary_data:
        print("ğŸ“­ æ²’æœ‰ç´…å­—æ‘˜è¦ï¼Œè·³éæ›´æ–°ã€‚")
        return
    print(f"\nğŸ“ æ­£åœ¨æ›´æ–°ä¸»é€±å ±æŒ‡å®šå€å¡Š: {main_report_data['title']}...")
    sys.stdout.flush()
    
    html_content = main_report_data['body']['storage']['value']
    soup = BeautifulSoup(html_content, 'lxml')
    SEPARATOR = "-------------------------------------"
    separators = []
    sep_pattern = re.compile(r'-{20,}')
    for tag in soup.find_all(string=sep_pattern):
        parent = tag.find_parent(['p', 'div'])
        if parent: separators.append(parent)
        else: separators.append(tag)
    
    target_start = None
    if len(separators) >= 2:
        print("   âœ… æ‰¾åˆ°ç¾æœ‰å€å¡Šï¼Œæº–å‚™æ¸…ç©ºä¸¦è¦†å¯«...")
        target_start = separators[-2]
        target_end = separators[-1]
        curr = target_start.next_sibling
        while curr and curr != target_end:
            next_node = curr.next_sibling
            if isinstance(curr, Tag) or isinstance(curr, NavigableString): curr.extract()
            curr = next_node
    else:
        print("   âš ï¸ æœªæ‰¾åˆ°å®Œæ•´å€å¡Šï¼Œå°‡åœ¨é é¢æœ€ä¸‹æ–¹æ–°å¢...")
        target_start = soup.new_tag('p'); target_start.string = SEPARATOR
        target_end = soup.new_tag('p'); target_end.string = SEPARATOR
        soup.append(target_start); soup.append(target_end)
    
    cursor = target_start
    for project_data in summary_data:
        p_name = project_data['project']
        p_items = project_data['items'] # é€™æ˜¯ä¸€å † list of nodes
        if not p_items: continue
        
        print(f"   ğŸ‘‰ [SUMMARY] å¯«å…¥å°ˆæ¡ˆ: {p_name}")
        sys.stdout.flush()
        
        name_tag = soup.new_tag('p')
        strong = soup.new_tag('strong'); strong.string = p_name
        name_tag.append(strong)
        cursor.insert_after(name_tag); cursor = name_tag
        
        # ç”±æ–¼ p_items ç¾åœ¨æ˜¯ä¿ç•™äº†å®Œæ•´çµæ§‹çš„ fragments
        # æˆ‘å€‘ä¸è¦ç”¨ <p> ç¡¬åŒ…ï¼Œè€Œæ˜¯ç”¨ <div> ä¿æŒçµæ§‹
        for entry_nodes in p_items:
            # entry_nodes æ˜¯ä¸€å€‹ listï¼Œè£¡é¢å¯èƒ½æ˜¯ <ul>, <p>, text ç­‰æ··åˆ
            container = soup.new_tag('div')
            for node in entry_nodes:
                container.append(copy.copy(node))
            
            cursor.insert_after(container); cursor = container
            
        spacer = soup.new_tag('p'); spacer.append(soup.new_tag('br'))
        cursor.insert_after(spacer); cursor = spacer

    print(f"ğŸ’¾ å„²å­˜ä¸»é€±å ±...")
    url = f"{API_ENDPOINT}/{main_report_data['id']}"
    payload = {
        "version": {"number": main_report_data['version']['number'] + 1, "minorEdit": True},
        "title": main_report_data['title'],
        "type": "page",
        "body": {"storage": {"value": str(soup), "representation": "storage"}}
    }
    requests.put(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), headers=get_headers(), data=json.dumps(payload)).raise_for_status()
    print("âœ… ä¸»é€±å ±æ›´æ–°æˆåŠŸï¼")

def main():
    print("=== Confluence Cleaner (V38: Clone & Prune) ===")
    main_report = find_latest_report()
    targets = extract_all_project_links(main_report['body']['view']['value'])
    if not targets: return
    print(f"ğŸ“‹ æ‰¾åˆ° {len(targets)} å€‹å°ˆæ¡ˆ")
    summary_collection = []
    for t in targets:
        print(f"\nğŸš€ {t['name']}")
        p = None
        if 'id' in t: p = get_page_by_id(t['id'])
        elif 'title' in t:
            print(f"   ä½¿ç”¨è§£ææ¨™é¡Œ: {t['title']}")
            p = get_page_by_title(t['title'])
        if not p: print("âŒ è®€å–å¤±æ•—"); continue
        
        new_c, red_items = clean_project_page_content(p['body']['storage']['value'], p['title'])
        if red_items:
            summary_collection.append({'project': t['name'], 'items': red_items})
        print("ğŸ‘Œ å°ˆæ¡ˆé é¢ç„¡éœ€è®Šæ›´ (å”¯è®€æ¨¡å¼)")

    print("-" * 30)
    if summary_collection: update_main_report_summary(main_report, summary_collection)
    else: print("ğŸ“­ æ²’æœ‰ç´…å­—æ‘˜è¦ï¼Œè·³éæ›´æ–°ã€‚")

if __name__ == "__main__":
    main()
