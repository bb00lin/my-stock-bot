import os
import requests
import json
import re
import sys
import copy
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup, Tag, NavigableString

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
KEEP_LIMIT = 5  # ä¿ç•™æœ€æ–°çš„å¹¾ç­†è³‡æ–™

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸ (CONF_URL, CONF_USER, CONF_PASS)")
    sys.exit(1)

parsed = urlparse(RAW_URL)
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

# --- 1. æœå°‹é€±å ±èˆ‡æå–æ‰€æœ‰å°ˆæ¡ˆé€£çµ (æ–°é‚è¼¯) ---
def find_latest_report():
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    url = f"{API_ENDPOINT}/search"
    params = {'cql': cql, 'limit': 1, 'expand': 'body.view'}
    response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    response.raise_for_status()
    results = response.json().get('results', [])
    if not results:
        print("âš ï¸ æ‰¾ä¸åˆ°é€±å ±")
        sys.exit(1)
    return results[0]

def extract_all_project_links(report_body):
    """
    è§£æé€±å ±å…§å®¹ï¼Œæ‰¾åˆ°å«æœ‰ 'Project' æ¬„ä½çš„è¡¨æ ¼ï¼Œ
    ä¸¦å›å‚³è©²æ¬„ä½ä¸­æ‰€æœ‰çš„é é¢é€£çµåˆ—è¡¨ã€‚
    """
    soup = BeautifulSoup(report_body, 'html.parser')
    tables = soup.find_all('table')
    
    project_targets = []
    found_table = False

    for table in tables:
        # 1. æª¢æŸ¥è¡¨é ­æ˜¯å¦å«æœ‰ "Project"
        headers = []
        header_row = table.find('tr')
        if not header_row: continue
        
        for cell in header_row.find_all(['th', 'td']):
            headers.append(cell.get_text().strip())
        
        if "Project" in headers:
            print("âœ… æ‰¾åˆ° Project Status è¡¨æ ¼ï¼Œé–‹å§‹è§£æå°ˆæ¡ˆé€£çµ...")
            found_table = True
            proj_idx = headers.index("Project")
            
            # 2. éæ­·è©²è¡¨æ ¼çš„æ‰€æœ‰åˆ—
            rows = table.find_all('tr')
            for row in rows[1:]: # è·³éè¡¨é ­
                cols = row.find_all('td')
                if len(cols) > proj_idx:
                    # æŠ“å–è©²æ ¼å…§æ‰€æœ‰çš„é€£çµ (å¯èƒ½æœ‰ç”±å¤šå€‹å°ˆæ¡ˆ)
                    links = cols[proj_idx].find_all('a')
                    for link in links:
                        page_id = link.get('data-linked-resource-id')
                        target = {}
                        
                        if page_id:
                            target['id'] = page_id
                            target['name'] = link.get_text().strip()
                        else:
                            # è™•ç†æ²’æœ‰ data-id çš„å‚³çµ±é€£çµ
                            href = link.get('href', '')
                            if 'pageId=' in href:
                                qs = parse_qs(urlparse(href).query)
                                if 'pageId' in qs: 
                                    target['id'] = qs['pageId'][0]
                                    target['name'] = link.get_text().strip()
                            else:
                                match = re.search(r'/pages/(\d+)/', href)
                                if match: 
                                    target['id'] = match.group(1)
                                    target['name'] = link.get_text().strip()
                                else:
                                    # å¦‚æœçœŸçš„æ‰¾ä¸åˆ° IDï¼Œå°±å­˜æ¨™é¡Œè®“å¾Œé¢å»çŒœ
                                    title = link.get_text().strip()
                                    if title:
                                        target['title'] = title
                                        target['name'] = title
                        
                        if target:
                            # é¿å…é‡è¤‡æ·»åŠ 
                            if target not in project_targets:
                                project_targets.append(target)
            
            # æ‰¾åˆ°ç¬¬ä¸€å€‹ç¬¦åˆçš„è¡¨æ ¼å°±åœæ­¢ï¼Œä¸å†å¾€ä¸‹æ‰¾ (é¿é–‹ Work Item Table)
            break
    
    if not found_table:
        print("âš ï¸ åœ¨é€±å ±ä¸­æ‰¾ä¸åˆ°å«æœ‰ 'Project' æ¬„ä½çš„è¡¨æ ¼ã€‚")
        
    return project_targets

def get_page_by_id(page_id):
    url = f"{API_ENDPOINT}/{page_id}"
    params = {'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    if resp.status_code == 200: return resp.json()
    return None

def get_page_by_title(title):
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    results = resp.json().get('results', [])
    if results: return results[0]
    
    # å˜—è©¦åŠ ä¸Š WeeklyStatus_ å‰ç¶´
    if not title.startswith("WeeklyStatus_"):
        alt_title = f"WeeklyStatus_{title}"
        print(f"   å˜—è©¦çŒœæ¸¬æ¨™é¡Œ: {alt_title}")
        params['title'] = alt_title
        resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        results = resp.json().get('results', [])
        if results: 
            print("   âœ… çŒœæ¸¬æˆåŠŸï¼")
            return results[0]
    return None

# --- 2. å…§å®¹è™•ç†é‚è¼¯ ---

def is_date_header(text):
    return bool(re.search(r'\[\d{4}/\d{1,2}/\d{1,2}\]', text))

def has_red_text(tag):
    if not isinstance(tag, Tag): return False
    if tag.has_attr('style'):
        style = tag['style'].lower()
        if 'rgb(255, 0, 0)' in style or '#ff0000' in style or 'red' in style: return True
    if tag.name == 'font' and (tag.get('color') == 'red' or tag.get('color') == '#ff0000'): return True
    for child in tag.descendants:
        if isinstance(child, Tag):
            if child.has_attr('style'):
                style = child['style'].lower()
                if 'rgb(255, 0, 0)' in style or '#ff0000' in style: return True
            if child.name == 'font' and (child.get('color') == 'red' or child.get('color') == '#ff0000'): return True
    return False

def get_clean_item_name(td_tag):
    """
    å¾ Item æ¬„ä½æå–ä¹¾æ·¨çš„åç¨±ã€‚
    éæ¿¾æ‰ Status Macro (complete/incomplete) å’Œåœ–ç‰‡ã€‚
    """
    # è¤‡è£½ä¸€ä»½ä»¥å…ä¿®æ”¹åˆ°åŸå§‹ soup
    temp_tag = copy.copy(td_tag)
    
    # ç§»é™¤ status macro
    for status in temp_tag.find_all('ac:structured-macro', attrs={"ac:name": "status"}):
        status.decompose()
        
    # ç§»é™¤ image (æœ‰çš„æ™‚å€™æœƒæœ‰ icon)
    for img in temp_tag.find_all('ac:image'):
        img.decompose()
        
    # å–å‡ºç´”æ–‡å­—ï¼Œä¸¦å°‡å¤šé¤˜çš„ç©ºç™½ç¸®æ¸›
    text = temp_tag.get_text(separator=' ', strip=True)
    
    # å¦‚æœæ¸…ç©ºå¾Œè®Šæ²’å­—äº† (ä¾‹å¦‚åŸæœ¬åªæœ‰ä¸€å€‹ status macro)ï¼Œé‚£é‚„æ˜¯å›å‚³åŸæœ¬çš„ï¼Œé¿å…ç©ºç™½
    if not text:
        return td_tag.get_text(strip=True)
        
    return text

def split_cell_content(cell_soup):
    entries = []
    current_entry = []
    for child in cell_soup.contents:
        if isinstance(child, NavigableString) and not child.strip():
            if current_entry: current_entry.append(child)
            continue
        text = child.get_text() if isinstance(child, Tag) else str(child)
        if is_date_header(text):
            if current_entry: entries.append(current_entry)
            current_entry = [child]
        else:
            current_entry.append(child)
    if current_entry: entries.append(current_entry)
    return entries

def check_entry_red(entry_nodes):
    for node in entry_nodes:
        if isinstance(node, Tag):
            if has_red_text(node): return True
    return False

def get_or_create_history_table(soup, main_table):
    macros = soup.find_all('ac:structured-macro', attrs={"ac:name": "expand"})
    target_macro = None
    
    for m in macros:
        title_param = m.find('ac:parameter', attrs={"ac:name": "title"})
        if title_param and "history" in title_param.get_text().lower():
            target_macro = m
            break
    
    if not target_macro:
        target_macro = soup.new_tag('ac:structured-macro', attrs={"ac:name": "expand"})
        p_title = soup.new_tag('ac:parameter', attrs={"ac:name": "title"})
        p_title.string = "history"
        target_macro.append(p_title)
        body = soup.new_tag('ac:rich-text-body')
        target_macro.append(body)
        
        if main_table.parent:
            main_table.insert_after(target_macro)
            target_macro.insert_before(soup.new_tag('p'))
    
    body = target_macro.find('ac:rich-text-body')
    hist_table = body.find('table')
    
    if not hist_table:
        hist_table = soup.new_tag('table')
        main_thead_row = main_table.find('tr')
        if main_thead_row:
            hist_table.append(copy.copy(main_thead_row))
        body.append(hist_table)
        
    return hist_table

def clean_project_page_content(html_content, page_title):
    soup = BeautifulSoup(html_content, 'html.parser')
    changed = False
    
    main_table = None
    all_tables = soup.find_all('table')
    
    for table in all_tables:
        if table.find_parent('ac:structured-macro'):
            continue
        headers = [th.get_text().strip() for th in table.find_all('th')]
        if "Item" in headers and "Update" in headers:
            main_table = table
            break
            
    if not main_table:
        print(f"   âš ï¸  [{page_title}] æ‰¾ä¸åˆ°ä¸»è¡¨æ ¼ (Item/Update)ï¼Œè·³éã€‚")
        return None

    print(f"   ğŸ” [{page_title}] æ‰¾åˆ°ä¸»è¡¨æ ¼ï¼Œåˆ†æä¸­...")
    
    tbody = main_table.find('tbody') or main_table
    rows = tbody.find_all('tr')
    
    header_row = rows[0]
    headers = [cell.get_text().strip() for cell in header_row.find_all(['th', 'td'])]
    try:
        item_idx = headers.index("Item")
        update_idx = headers.index("Update")
    except ValueError:
        return None

    history_table_ref = None

    for row in rows[1:]:
        cols = row.find_all('td')
        if len(cols) <= max(item_idx, update_idx): continue
        
        # ä½¿ç”¨æ–°çš„å‡½å¼å–å¾—ä¹¾æ·¨çš„ Item Name
        raw_item_cell = cols[item_idx]
        clean_item_name = get_clean_item_name(raw_item_cell)
        
        update_cell = cols[update_idx]
        
        entries = split_cell_content(update_cell)
        if len(entries) <= KEEP_LIMIT:
            continue
            
        print(f"      Item [{clean_item_name}]: ç™¼ç¾ {len(entries)} ç­†ç´€éŒ„ï¼Œæ­£åœ¨æ¸…ç†...")
        
        keep_entries = []
        archive_entries = []
        count = 0
        for entry in entries:
            is_red = check_entry_red(entry)
            if is_red:
                keep_entries.append(entry)
                continue
            
            if count < KEEP_LIMIT:
                keep_entries.append(entry)
                count += 1
            else:
                archive_entries.append(entry)
        
        if not archive_entries:
            continue
            
        print(f"      âœ‚ï¸  æ¬ç§» {len(archive_entries)} ç­†èˆŠè³‡æ–™åˆ° History...")
        changed = True
        
        update_cell.clear()
        for entry in keep_entries:
            for node in entry:
                update_cell.append(node)
                
        if history_table_ref is None:
            history_table_ref = get_or_create_history_table(soup, main_table)
            
        hist_rows = history_table_ref.find_all('tr')
        target_hist_row = None
        
        # åœ¨ History è¡¨æ ¼ä¸­æ¯”å° Item Name (åŒæ¨£ç”¨ä¹¾æ·¨åç¨±æ¯”å°)
        for h_row in hist_rows:
            h_cols = h_row.find_all('td')
            if not h_cols: continue
            
            h_item_name = get_clean_item_name(h_cols[item_idx])
            if h_item_name == clean_item_name:
                target_hist_row = h_row
                break
        
        if not target_hist_row:
            target_hist_row = soup.new_tag('tr')
            for _ in range(len(headers)):
                target_hist_row.append(soup.new_tag('td'))
            
            # åœ¨æ–°çš„ä¸€åˆ—å¡«å…¥ä¹¾æ·¨çš„åç¨± (ä¸åŒ…å« status macro)
            target_hist_row.find_all('td')[item_idx].string = clean_item_name
            history_table_ref.append(target_hist_row)
            
        hist_update_cell = target_hist_row.find_all('td')[update_idx]
        if hist_update_cell.contents:
            hist_update_cell.append(soup.new_tag('br'))
            
        for entry in archive_entries:
            for node in entry:
                hist_update_cell.append(node)

    return str(soup) if changed else None

def update_page(page_data, new_content):
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜: {page_data['title']} (éœé»˜æ¨¡å¼)...")
    url = f"{API_ENDPOINT}/{page_data['id']}"
    payload = {
        "version": {"number": page_data['version']['number'] + 1, "minorEdit": True},
        "title": page_data['title'],
        "type": "page",
        "body": {
            "storage": {
                "value": new_content,
                "representation": "storage"
            }
        }
    }
    resp = requests.put(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), 
                       headers=get_headers(), data=json.dumps(payload))
    resp.raise_for_status()
    print("âœ… æ›´æ–°æˆåŠŸï¼")

def main():
    print("=== Confluence å°ˆæ¡ˆé é¢æ•´ç†æ©Ÿå™¨äºº (V4: Batch & Clean) ===")
    
    report = find_latest_report()
    
    # 1. æŠ“å–æ‰€æœ‰ç›®æ¨™å°ˆæ¡ˆ
    project_targets = extract_all_project_links(report['body']['view']['value'])
    
    if not project_targets:
        print("çµæŸï¼šæ²’æœ‰æ‰¾åˆ°ä»»ä½•å°ˆæ¡ˆé€£çµã€‚")
        return

    print(f"ğŸ“‹ ç¸½å…±æ‰¾åˆ° {len(project_targets)} å€‹å°ˆæ¡ˆç›®æ¨™ï¼š{[p['name'] for p in project_targets]}")
    print("-" * 30)

    # 2. æ‰¹é‡è™•ç†
    for target in project_targets:
        print(f"\nğŸš€ é–‹å§‹è™•ç†å°ˆæ¡ˆ: {target['name']}")
        
        page_data = None
        if 'id' in target:
            page_data = get_page_by_id(target['id'])
        elif 'title' in target:
            page_data = get_page_by_title(target['title'])
            
        if not page_data:
            print(f"âŒ ç„¡æ³•è®€å–é é¢ï¼Œè·³éã€‚")
            continue
            
        new_content = clean_project_page_content(page_data['body']['storage']['value'], page_data['title'])
        
        if new_content:
            update_page(page_data, new_content)
        else:
            print("ğŸ‘Œ é é¢ç„¡éœ€è®Šæ›´")

if __name__ == "__main__":
    main()
