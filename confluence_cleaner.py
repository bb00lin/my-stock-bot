import os
import requests
import json
import re
import sys
import copy
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup, Tag, NavigableString

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
KEEP_LIMIT = 5  # ä¿ç•™æœ€æ–°çš„å¹¾ç­†è³‡æ–™

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

parsed = urlparse(RAW_URL)
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

# --- 1. æœå°‹é€±å ±èˆ‡å°ˆæ¡ˆé€£çµ (ç¶­æŒä¸è®Š) ---
def find_latest_report():
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    url = f"{API_ENDPOINT}/search"
    params = {'cql': cql, 'limit': 1, 'expand': 'body.view'}
    response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    response.raise_for_status()
    results = response.json().get('results', [])
    if not results:
        print("âš ï¸ æ‰¾ä¸åˆ°é€±å ±")
        sys.exit(1)
    return results[0]

def extract_first_project_link(report_body):
    soup = BeautifulSoup(report_body, 'html.parser')
    tables = soup.find_all('table')
    for table in tables:
        headers = []
        header_row = table.find('tr')
        if not header_row: continue
        for cell in header_row.find_all(['th', 'td']):
            headers.append(cell.get_text().strip())
        
        if "Project" in headers:
            proj_idx = headers.index("Project")
            rows = table.find_all('tr')
            for row in rows[1:]:
                cols = row.find_all('td')
                if len(cols) > proj_idx:
                    link_tag = cols[proj_idx].find('a')
                    if link_tag:
                        page_id = link_tag.get('data-linked-resource-id')
                        if page_id:
                            print(f"ğŸ¯ é–å®šç›®æ¨™ (é€é data-id): {page_id}")
                            return {'id': page_id}
                        href = link_tag.get('href', '')
                        if 'pageId=' in href:
                            qs = parse_qs(urlparse(href).query)
                            if 'pageId' in qs: return {'id': qs['pageId'][0]}
                        match = re.search(r'/pages/(\d+)/', href)
                        if match: return {'id': match.group(1)}
                        title = link_tag.get_text().strip()
                        print(f"âš ï¸ è­¦å‘Šï¼šç„¡æ³•è§£æ IDï¼Œä½¿ç”¨æ¨™é¡Œ: {title}")
                        return {'title': title}
    print("âš ï¸ æ‰¾ä¸åˆ° Project é€£çµ")
    return None

def get_page_by_id(page_id):
    url = f"{API_ENDPOINT}/{page_id}"
    params = {'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    if resp.status_code == 200: return resp.json()
    return None

def get_page_by_title(title):
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    results = resp.json().get('results', [])
    if results: return results[0]
    if not title.startswith("WeeklyStatus_"):
        alt_title = f"WeeklyStatus_{title}"
        print(f"   å˜—è©¦çŒœæ¸¬æ¨™é¡Œ: {alt_title}")
        params['title'] = alt_title
        resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        results = resp.json().get('results', [])
        if results: 
            print("   âœ… çŒœæ¸¬æˆåŠŸï¼")
            return results[0]
    return None

# --- 2. å…§å®¹è™•ç†é‚è¼¯ (å¤§å¹…ä¿®æ”¹) ---

def is_date_header(text):
    """æª¢æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼ [YYYY/MM/DD]"""
    # å¯¬é¬†åŒ¹é…ï¼šåªè¦æœ‰ [æ•¸å­—/æ•¸å­—/æ•¸å­—] å°±ç•¶ä½œæ˜¯é–‹é ­
    return bool(re.search(r'\[\d{4}/\d{1,2}/\d{1,2}\]', text))

def has_red_text(tag):
    """æª¢æŸ¥é€™å€‹æ¨™ç±¤(åŒ…å«å­æ¨™ç±¤)æ˜¯å¦æœ‰ç´…å­—"""
    if not isinstance(tag, Tag): return False
    # æª¢æŸ¥ style
    if tag.has_attr('style'):
        style = tag['style'].lower()
        if 'rgb(255, 0, 0)' in style or '#ff0000' in style or 'red' in style:
            return True
    # æª¢æŸ¥ font tag
    if tag.name == 'font' and (tag.get('color') == 'red' or tag.get('color') == '#ff0000'):
        return True
    # éè¿´æª¢æŸ¥å­ç¯€é»
    for child in tag.descendants:
        if isinstance(child, Tag):
            if child.has_attr('style'):
                style = child['style'].lower()
                if 'rgb(255, 0, 0)' in style or '#ff0000' in style: return True
            if child.name == 'font' and (child.get('color') == 'red' or child.get('color') == '#ff0000'):
                return True
    return False

def split_cell_content(cell_soup):
    """å°‡æ ¼å­å…§çš„å…§å®¹åˆ‡åˆ†æˆä¸€å€‹å€‹ Entry (ä»¥æ—¥æœŸé–‹é ­ç‚ºç•Œ)"""
    entries = []
    current_entry = []
    
    # Confluence Storage Format é€šå¸¸æ˜¯ <p>[Date]</p><ul>...</ul> æˆ–è€…æ˜¯ <p>[Date]<br/>...</p>
    # æˆ‘å€‘éæ­·æ‰€æœ‰å­ç¯€é»
    for child in cell_soup.contents:
        if isinstance(child, NavigableString) and not child.strip():
            # ç©ºç™½å­—ä¸²ï¼Œé™„å±¬åœ¨ä¸Šä¸€æ®µ
            if current_entry: current_entry.append(child)
            continue
            
        text = child.get_text() if isinstance(child, Tag) else str(child)
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºæ–°çš„æ—¥æœŸé–‹é ­
        # 1. å¿…é ˆå«æœ‰æ—¥æœŸæ ¼å¼
        # 2. é€šå¸¸æ—¥æœŸæ˜¯ç¨ç«‹çš„ä¸€è¡Œ (P tag) æˆ–æ˜¯æ–‡å­—çš„é–‹é ­
        if is_date_header(text):
            # å„²å­˜ä¸Šä¸€ç­†
            if current_entry:
                entries.append(current_entry)
            # é–‹å•Ÿæ–°çš„ä¸€ç­†
            current_entry = [child]
        else:
            # ä¸æ˜¯æ—¥æœŸé–‹é ­ï¼Œæ­¸å…¥ç•¶å‰é€™ä¸€ç­†
            # å¦‚æœé‚„æ²’æœ‰ä»»ä½•æ—¥æœŸé–‹é ­(æœ€ä¸Šé¢çš„é›œè¨Š)ï¼Œä¹Ÿå…ˆæ­¸å…¥ current
            current_entry.append(child)
            
    # æœ€å¾Œä¸€ç­†
    if current_entry:
        entries.append(current_entry)
        
    return entries

def check_entry_red(entry_nodes):
    """æª¢æŸ¥é€™ä¸€æ•´ç­† Entry è£¡é¢æœ‰æ²’æœ‰ç´…å­—"""
    for node in entry_nodes:
        if isinstance(node, Tag):
            if has_red_text(node): return True
    return False

def clean_project_page_content(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    changed = False
    
    # 1. ç¢ºä¿æœ‰ History æ¨™é¡Œ
    history_header = soup.find(lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4'] and 'History' in tag.get_text())
    if not history_header:
        print("   â„¹ï¸ å»ºç«‹ History å€å¡Š...")
        history_header = soup.new_tag('h2')
        history_header.string = "History"
        soup.append(history_header)
        changed = True

    # 2. æ‰¾åˆ°ä¸»è¡¨æ ¼ (åˆ¤æ–·ä¾æ“š: è¡¨é ­æœ‰ Item å’Œ Update)
    main_table = None
    all_tables = soup.find_all('table')
    
    for table in all_tables:
        # æª¢æŸ¥è¡¨é ­
        headers = [th.get_text().strip() for th in table.find_all('th')]
        if "Item" in headers and "Update" in headers:
            # ä¸”é€™å€‹è¡¨æ ¼è¦åœ¨ History ä¹‹å‰ (å¦‚æœæœ‰ History çš„è©±)
            if history_header and table.sourceline and history_header.sourceline:
                if table.sourceline > history_header.sourceline: continue
            main_table = table
            break
            
    if not main_table:
        print("   âš ï¸ æ‰¾ä¸åˆ°ä¸»è¡¨æ ¼ (Item/Update)ï¼Œè·³éè™•ç†")
        return str(soup) if changed else None

    print("   ğŸ” æ‰¾åˆ°ä¸»è¡¨æ ¼ï¼Œé–‹å§‹åˆ†æ Rows...")
    
    # 3. è™•ç†ä¸»è¡¨æ ¼çš„æ¯ä¸€åˆ—
    tbody = main_table.find('tbody') or main_table
    rows = tbody.find_all('tr')
    
    # æ‰¾å‡ºæ¬„ä½ç´¢å¼•
    header_row = rows[0]
    headers = [cell.get_text().strip() for cell in header_row.find_all(['th', 'td'])]
    try:
        item_idx = headers.index("Item")
        update_idx = headers.index("Update")
    except ValueError:
        return str(soup) if changed else None

    # æº–å‚™ History è¡¨æ ¼ (å¦‚æœéœ€è¦æ¬ç§»æ‰ç”¨åˆ°)
    history_table = None
    
    for row in rows[1:]: # è·³éè¡¨é ­
        cols = row.find_all('td')
        if len(cols) <= max(item_idx, update_idx): continue
        
        item_name = cols[item_idx].get_text().strip()
        update_cell = cols[update_idx]
        
        # A. åˆ‡å‰²å…§å®¹
        entries = split_cell_content(update_cell)
        if len(entries) <= KEEP_LIMIT:
            continue # æ•¸é‡æœªé”æ¨™ï¼Œè·³é
            
        print(f"      Item [{item_name}]: å…±æœ‰ {len(entries)} ç­†ç´€éŒ„ï¼Œæº–å‚™æ¸…ç†...")
        
        # B. ç¯©é¸ (ä¿ç•™ vs æ­¸æª”)
        keep_entries = []
        archive_entries = []
        
        count = 0
        for entry in entries:
            is_red = check_entry_red(entry)
            if is_red:
                keep_entries.append(entry)
                # print("         ğŸ”´ ç´…å­—ä¿ç•™")
                continue
            
            if count < KEEP_LIMIT:
                keep_entries.append(entry)
                count += 1
            else:
                archive_entries.append(entry)
        
        if not archive_entries:
            continue
            
        print(f"      âœ‚ï¸ å°‡æ­¸æª” {len(archive_entries)} ç­†è³‡æ–™...")
        changed = True
        
        # C. æ›´æ–°ä¸»è¡¨æ ¼ (æ¸…ç©º -> å¡«å…¥ä¿ç•™çš„)
        update_cell.clear()
        for entry in keep_entries:
            for node in entry:
                update_cell.append(node)
                
        # D. è™•ç† History
        # å°‹æ‰¾ History è¡¨æ ¼ (åœ¨ History header ä¹‹å¾Œ)
        if not history_table:
            # å˜—è©¦å°‹æ‰¾æ—¢æœ‰çš„
            curr = history_header.next_sibling
            while curr:
                if isinstance(curr, Tag) and curr.name == 'table':
                    # æª¢æŸ¥è¡¨é ­æ˜¯å¦æ­£ç¢º
                    h_headers = [th.get_text().strip() for th in curr.find_all('th')]
                    if "Item" in h_headers and "Update" in h_headers:
                        history_table = curr
                        break
                curr = curr.next_sibling
            
            # å¦‚æœé‚„æ˜¯æ²’æœ‰ï¼Œå°±æ–°å»ºä¸€å€‹
            if not history_table:
                print("      ğŸ†• æ–°å»º History è¡¨æ ¼...")
                history_table = soup.new_tag('table')
                # è¤‡è£½è¡¨é ­
                new_thead = copy.copy(rows[0])
                history_table.append(new_thead)
                # æ’å…¥åˆ° History header ä¹‹å¾Œ
                history_header.insert_after(history_table)
        
        # åœ¨ History è¡¨æ ¼ä¸­æ‰¾å°æ‡‰ Item çš„ Row
        hist_rows = history_table.find_all('tr')
        target_hist_row = None
        
        for h_row in hist_rows:
            h_cols = h_row.find_all('td')
            if not h_cols: continue
            if h_cols[item_idx].get_text().strip() == item_name:
                target_hist_row = h_row
                break
        
        if not target_hist_row:
            # æ²’æ‰¾åˆ°ï¼Œæ–°å»ºä¸€è¡Œ
            target_hist_row = soup.new_tag('tr')
            # è£œæ»¿æ ¼å­
            for _ in range(len(headers)):
                target_hist_row.append(soup.new_tag('td'))
            # å¡«å…¥ Item Name
            target_hist_row.find_all('td')[item_idx].string = item_name
            history_table.append(target_hist_row)
            
        # å°‡è³‡æ–™å¡å…¥ History çš„ Update æ¬„ä½
        hist_update_cell = target_hist_row.find_all('td')[update_idx]
        
        # åœ¨å¡å…¥å‰ï¼Œæœ€å¥½åŠ å€‹åˆ†éš” (ä¾‹å¦‚æ›è¡Œ)
        if hist_update_cell.contents:
            hist_update_cell.append(soup.new_tag('br'))
            
        for entry in archive_entries:
            for node in entry:
                # æ³¨æ„ï¼šé€™è£¡è¦ copy ç¯€é»ï¼Œå› ç‚ºåŸç¯€é»å·²ç¶“å¾ä¸»è¡¨æ ¼æ‹”é™¤
                # ä½† append æœƒè‡ªå‹•è™•ç†ç§»å‹•ï¼Œæ‰€ä»¥ç›´æ¥ append å³å¯
                hist_update_cell.append(node)

    return str(soup) if changed else None

def update_page(page_data, new_content):
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜é é¢: {page_data['title']} (éœé»˜æ¨¡å¼)...")
    url = f"{API_ENDPOINT}/{page_data['id']}"
    payload = {
        "version": {"number": page_data['version']['number'] + 1, "minorEdit": True},
        "title": page_data['title'],
        "type": "page",
        "body": {
            "storage": {
                "value": new_content,
                "representation": "storage"
            }
        }
    }
    resp = requests.put(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), 
                       headers=get_headers(), data=json.dumps(payload))
    resp.raise_for_status()
    print("âœ… æ›´æ–°æˆåŠŸï¼")

def main():
    print("=== Confluence å°ˆæ¡ˆé é¢æ•´ç†æ©Ÿå™¨äºº (V2: Cell Parsing) ===")
    
    report = find_latest_report()
    target_info = extract_first_project_link(report['body']['view']['value'])
    
    if not target_info:
        print("çµæŸï¼šæ²’æœ‰æ‰¾åˆ°å¯è™•ç†çš„å°ˆæ¡ˆé€£çµã€‚")
        return

    if 'id' in target_info:
        page_data = get_page_by_id(target_info['id'])
    else:
        page_data = get_page_by_title(target_info['title'])
        
    if not page_data:
        print(f"âŒ æœ€çµ‚å¤±æ•—ï¼šç„¡æ³•æ‰¾åˆ°å°æ‡‰é é¢")
        return
        
    print(f"ğŸ“– è®€å–é é¢: {page_data['title']} (ID: {page_data['id']})")
    
    new_content = clean_project_page_content(page_data['body']['storage']['value'])
    
    if new_content:
        update_page(page_data, new_content)
    else:
        print("ğŸ‘Œ é é¢ç„¡éœ€è®Šæ›´")

if __name__ == "__main__":
    main()
