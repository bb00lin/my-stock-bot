import os
import requests
import json
import re
import sys
import copy
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup, Tag, NavigableString

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
MASTER_PAGE_ID = os.environ.get("MASTER_PAGE_ID")
KEEP_LIMIT = 5 

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

parsed = urlparse(RAW_URL)
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

# --- 1. æœå°‹é€±å ± ---
def find_latest_report():
    if MASTER_PAGE_ID:
        print(f"ğŸ¯ åµæ¸¬åˆ° MASTER_PAGE_ID ({MASTER_PAGE_ID})")
        url = f"{API_ENDPOINT}/{MASTER_PAGE_ID}"
        params = {'expand': 'body.view,version'}
        try:
            r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
            r.raise_for_status()
            return r.json()
        except Exception as e:
            print(f"âŒ è®€å–å¤±æ•—: {e}")
            sys.exit(1)

    print("ğŸ” æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    url = f"{API_ENDPOINT}/search"
    params = {'cql': cql, 'limit': 1, 'expand': 'body.view'}
    r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    r.raise_for_status()
    results = r.json().get('results', [])
    if not results:
        print("âš ï¸ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°é€±å ±")
        sys.exit(1)
    print(f"âœ… æœå°‹æˆåŠŸ: {results[0]['title']}")
    return results[0]

def extract_all_project_links(report_body):
    soup = BeautifulSoup(report_body, 'lxml')
    tables = soup.find_all('table')
    project_targets = []
    
    for table in tables:
        h_row = table.find('tr')
        if not h_row: continue
        headers = [c.get_text().strip() for c in h_row.find_all(['th', 'td'])]
        
        if "Project" in headers:
            print("âœ… æ‰¾åˆ° Project Status è¡¨æ ¼")
            proj_idx = headers.index("Project")
            for row in table.find_all('tr')[1:]:
                cols = row.find_all('td')
                if len(cols) > proj_idx:
                    for link in cols[proj_idx].find_all('a'):
                        pid = link.get('data-linked-resource-id')
                        name = link.get_text().strip()
                        target = {'name': name}
                        if pid:
                            target['id'] = pid
                        else:
                            href = link.get('href', '')
                            if 'pageId=' in href:
                                qs = parse_qs(urlparse(href).query)
                                if 'pageId' in qs: target['id'] = qs['pageId'][0]
                            else:
                                m = re.search(r'/pages/(\d+)/', href)
                                if m: target['id'] = m.group(1)
                                else: target['title'] = name
                        
                        if target.get('id') or target.get('title'):
                            if target not in project_targets: project_targets.append(target)
            break 
    return project_targets

def get_page_by_id(page_id):
    url = f"{API_ENDPOINT}/{page_id}"
    params = {'expand': 'body.storage,version'}
    r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    return r.json() if r.status_code == 200 else None

def get_page_by_title(title):
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version'}
    r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    res = r.json().get('results', [])
    if res: return res[0]
    if not title.startswith("WeeklyStatus_"):
        r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params={'title': f"WeeklyStatus_{title}", 'expand': 'body.storage,version'})
        res = r.json().get('results', [])
        if res: return res[0]
    return None

# --- V16 æ ¸å¿ƒï¼šçµæ§‹å¿«ç¯©é˜²ç¦¦æ©Ÿåˆ¶ ---

def is_date_header(text):
    if not text: return False
    # åªå–å‰ 50 å­—å…ƒæª¢æŸ¥ï¼Œé¿å… regex å¡æ­»
    return bool(re.search(r'\[\d{4}/\d{1,2}/\d{1,2}\]', text[:50]))

def is_safe_to_read_text(tag):
    """
    ã€V16 æ ¸å¿ƒã€‘æ±ºå®šä¸€å€‹æ¨™ç±¤æ˜¯å¦ã€Œå®‰å…¨ã€åˆ°å¯ä»¥è®€å–æ–‡å­—ã€‚
    å¦‚æœæ¨™ç±¤å…§åŒ…å«å¤§è¡¨æ ¼ï¼Œè®€å–æ–‡å­—æœƒè§¸ç™¼éæ­·ï¼Œå°è‡´å¡é “ã€‚
    """
    # 1. é»‘åå–®ï¼šé€™äº›æ¨™ç±¤çµ•å°ä¸æ˜¯æ¨™é¡Œ
    BLOCK_TAGS = ['table', 'tbody', 'thead', 'tr', 'td', 'ul', 'ol', 'ac:structured-macro', 'ac:layout-section']
    if tag.name in BLOCK_TAGS:
        return False
    
    # 2. çµæ§‹å¿«ç¯©ï¼šæª¢æŸ¥ç›´æ¥å­ç¯€é»
    # å¦‚æœç›´æ¥å­ç¯€é»åŒ…å«é‡å‹æ¨™ç±¤ï¼Œå‰‡åˆ¤å®šæ­¤æ¨™ç±¤ç‚ºã€Œå®¹å™¨ã€ï¼Œä¸è®€å–æ–‡å­—
    for child in tag.children:
        if isinstance(child, Tag):
            if child.name in BLOCK_TAGS:
                return False
            # é¡å¤–æª¢æŸ¥ï¼šå¦‚æœæ˜¯ div åŒ… div åŒ… table çš„æƒ…æ³
            if child.name in ['div', 'p']:
                # é€™è£¡åªåšæ·ºå±¤æª¢æŸ¥ï¼Œå¦‚æœé‚„æœ‰å­«ç¯€é»æ˜¯ tableï¼Œä¹Ÿæ”¾æ£„
                # find(recursive=False) é€Ÿåº¦æ¥µå¿«
                if child.find(BLOCK_TAGS, recursive=False):
                    return False

    # 3. æ•¸é‡é™åˆ¶ï¼šå¦‚æœå­ç¯€é»å¤ªå¤šï¼Œå¯èƒ½ä¹Ÿæ˜¯å¤§å…§å®¹ï¼Œè·³é
    # é€™è£¡è½‰ list æœƒæœ‰å¾®å°æˆæœ¬ï¼Œä½†åœ¨å¤§è¡¨æ ¼é¢å‰æ˜¯æ•‘å‘½ç¨»è‰
    # ä½¿ç”¨ sum(1 for _) é¿å…å»ºç«‹ list ä½”ç”¨è¨˜æ†¶é«”
    child_count = sum(1 for _ in tag.children)
    if child_count > 20: 
        return False

    return True

def split_cell_content(cell_soup):
    entries = []
    current_entry = []
    
    for child in cell_soup.contents:
        # 1. å¿½ç•¥ç´”ç©ºç™½
        if isinstance(child, NavigableString) and not child.strip():
            if current_entry: current_entry.append(child)
            continue
        
        is_header = False
        
        if isinstance(child, Tag):
            # ã€V16 ä¿®æ­£ã€‘ï¼šå…ˆåšçµæ§‹å¿«ç¯©ï¼Œç¢ºèªå®‰å…¨æ‰è®€æ–‡å­—
            if is_safe_to_read_text(child):
                # é€™è£¡è®€å–æ–‡å­—ç›¸å°å®‰å…¨
                txt = child.get_text().strip()
                if is_date_header(txt):
                    is_header = True
            else:
                # ä¸å®‰å…¨ï¼ˆåŒ…å«è¡¨æ ¼ç­‰ï¼‰ï¼Œç›´æ¥è¦–ç‚ºå…§å®¹ï¼Œis_header = False
                pass
        
        elif isinstance(child, NavigableString):
            if is_date_header(str(child).strip()):
                is_header = True

        if is_header:
            if current_entry: entries.append(current_entry)
            current_entry = [child]
        else:
            current_entry.append(child)
            
    if current_entry: entries.append(current_entry)
    return entries

# --- ç´…å­—æª¢æŸ¥ï¼šä¾ç„¶ä¿æŒå®‰å…¨æ¨¡å¼ ---
def is_red_style(tag):
    if tag.has_attr('style'):
        s = tag['style'].lower()
        if 'rgb(255, 0, 0)' in s or '#ff0000' in s or 'color: red' in s: return True
    if tag.name == 'font' and (tag.get('color') == 'red' or tag.get('color') == '#ff0000'): return True
    return False

def has_red_text_safe(tag):
    if not isinstance(tag, Tag): return False
    if is_red_style(tag): return True
    
    # ç¦å€ï¼šçµ•å°ä¸é€²å…¥å¤§è¡¨æ ¼æª¢æŸ¥ç´…å­—
    NO_GO = ['table', 'ac:structured-macro', 'tbody', 'thead', 'tr', 'td']
    if tag.name in NO_GO: return False

    for child in tag.children:
        if isinstance(child, Tag):
            if has_red_text_safe(child): return True
    return False

def check_entry_red(entry_nodes):
    for node in entry_nodes:
        if isinstance(node, Tag):
            if has_red_text_safe(node): return True
    return False

def get_or_create_history_table(soup, main_table):
    macros = soup.find_all('ac:structured-macro', attrs={"ac:name": "expand"})
    target_macro = None
    for m in macros:
        t = m.find('ac:parameter', attrs={"ac:name": "title"})
        if t and "history" in t.get_text().lower():
            target_macro = m
            break
    
    if not target_macro:
        target_macro = soup.new_tag('ac:structured-macro', attrs={"ac:name": "expand"})
        p = soup.new_tag('ac:parameter', attrs={"ac:name": "title"})
        p.string = "history"
        target_macro.append(p)
        body = soup.new_tag('ac:rich-text-body')
        target_macro.append(body)
        if main_table.parent:
            main_table.insert_after(target_macro)
            target_macro.insert_before(soup.new_tag('p'))
    
    body = target_macro.find('ac:rich-text-body')
    hist_table = body.find('table')
    if not hist_table:
        hist_table = soup.new_tag('table')
        thead = main_table.find('tr', recursive=False)
        if thead: hist_table.append(copy.copy(thead))
        body.append(hist_table)
    return hist_table

def clean_project_page_content(html_content, page_title):
    soup = BeautifulSoup(html_content, 'lxml')
    changed = False
    
    main_table = None
    all_tables = soup.find_all('table')
    for t in all_tables:
        if t.find_parent('ac:structured-macro'): continue
        headers = [c.get_text().strip() for c in t.find_all('th')]
        if "Item" in headers and "Update" in headers:
            main_table = t
            break
            
    if not main_table:
        print(f"   âš ï¸  [{page_title}] æ‰¾ä¸åˆ°ä¸»è¡¨æ ¼ï¼Œè·³éã€‚")
        return None

    print(f"   ğŸ” [{page_title}] æ‰¾åˆ°ä¸»è¡¨æ ¼ï¼Œåˆ†æä¸­...")
    sys.stdout.flush()
    
    # ä½¿ç”¨ main_table ç›´æ¥æ‰¾ tr (å…¼å®¹æœ‰ç„¡ tbody çš„æƒ…æ³)
    # recursive=False æ˜¯é—œéµ
    rows = main_table.find_all('tr', recursive=False)
    if not rows and main_table.find('tbody', recursive=False):
        rows = main_table.find('tbody', recursive=False).find_all('tr', recursive=False)

    if not rows: return None

    header_row = rows[0]
    headers = [c.get_text().strip() for c in header_row.find_all(['th', 'td'], recursive=False)]
    try:
        item_idx = headers.index("Item")
        update_idx = headers.index("Update")
    except ValueError: return None

    history_table_ref = None
    total_rows = len(rows) - 1
    
    for i, row in enumerate(rows[1:]):
        # æ¯ä¸€è¡Œéƒ½å°ï¼Œç¢ºä¿æ²’æ­»
        sys.stdout.write(f"\r      Processing Row {i+1}/{total_rows} ...")
        sys.stdout.flush()

        cols = row.find_all('td', recursive=False)
        if len(cols) <= max(item_idx, update_idx): continue
        
        # å®‰å…¨å–å
        item_name_tag = cols[item_idx]
        # åŒæ¨£ä½¿ç”¨å®‰å…¨æª¢æŸ¥
        if is_safe_to_read_text(item_name_tag):
            item_name = item_name_tag.get_text().strip()[:50]
        else:
            item_name = "Complex Item Name"

        update_cell = cols[update_idx]
        
        # V16 åŸ·è¡Œçµæ§‹å¿«ç¯©åˆ‡å‰²
        entries = split_cell_content(update_cell)
        
        if len(entries) <= KEEP_LIMIT: continue
            
        keep = []
        archive = []
        count = 0
        
        for entry in entries:
            # ç´…å­—æª¢æŸ¥
            if check_entry_red(entry):
                keep.append(entry)
                continue
            
            if count < KEEP_LIMIT:
                keep.append(entry)
                count += 1
            else:
                archive.append(entry)
        
        if not archive: continue
        changed = True
        
        update_cell.clear()
        for e in keep:
            for n in e: update_cell.append(n)
            
        if not history_table_ref:
            history_table_ref = get_or_create_history_table(soup, main_table)
            
        hist_rows = history_table_ref.find_all('tr', recursive=False)
        target_row = None
        for hr in hist_rows:
            hc = hr.find_all('td', recursive=False)
            if not hc: continue
            
            # é€™è£¡ä¹Ÿè¦å®‰å…¨è®€å–
            h_name = ""
            if is_safe_to_read_text(hc[item_idx]):
                h_name = hc[item_idx].get_text().strip()[:50]
                
            if h_name == item_name:
                target_row = hr
                break
        
        if not target_row:
            target_row = soup.new_tag('tr')
            for _ in range(len(headers)): target_row.append(soup.new_tag('td'))
            target_row.find_all('td')[item_idx].string = item_name
            history_table_ref.append(target_row)
            
        dest = target_row.find_all('td', recursive=False)[update_idx]
        if dest.contents: dest.append(soup.new_tag('br'))
        for e in archive:
            for n in e: dest.append(n)
    
    print(f"\r      Processing Row {total_rows}/{total_rows} (Done)        ")
    sys.stdout.flush()
    return str(soup) if changed else None

def update_page(page_data, new_content):
    print(f"ğŸ’¾ å„²å­˜: {page_data['title']}...")
    url = f"{API_ENDPOINT}/{page_data['id']}"
    payload = {
        "version": {"number": page_data['version']['number'] + 1, "minorEdit": True},
        "title": page_data['title'],
        "type": "page",
        "body": {"storage": {"value": new_content, "representation": "storage"}}
    }
    requests.put(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), headers=get_headers(), data=json.dumps(payload)).raise_for_status()
    print("âœ… æˆåŠŸï¼")

def main():
    print("=== Confluence Cleaner (V16: Structure Quick-Scan) ===")
    report = find_latest_report()
    targets = extract_all_project_links(report['body']['view']['value'])
    if not targets: return
    print(f"ğŸ“‹ æ‰¾åˆ° {len(targets)} å€‹å°ˆæ¡ˆ")
    for t in targets:
        print(f"\nğŸš€ {t['name']}")
        p = get_page_by_id(t['id']) if 'id' in t else get_page_by_title(t['title'])
        if not p:
            print("âŒ è®€å–å¤±æ•—")
            continue
        new_c = clean_project_page_content(p['body']['storage']['value'], p['title'])
        if new_c: update_page(p, new_c)
        else: print("ğŸ‘Œ ç„¡éœ€è®Šæ›´")

if __name__ == "__main__":
    main()