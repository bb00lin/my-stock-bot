import os
import requests
import json
import re
import sys
from datetime import datetime
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse
from bs4 import BeautifulSoup, Tag

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
KEEP_LIMIT = 5  # ä¿ç•™æœ€æ–°çš„å¹¾ç­†è³‡æ–™

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

parsed = urlparse(RAW_URL)
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

def find_latest_report():
    """æ‰¾åˆ°æœ€æ–°çš„é€±å ±ï¼Œä¸¦æŠ“å– View æ ¼å¼ (ç‚ºäº†çœ‹è¦‹ Macro ç”¢ç”Ÿçš„è¡¨æ ¼)"""
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    url = f"{API_ENDPOINT}/search"
    # ä¿®æ”¹é»ï¼šé€™è£¡æ”¹æŠ“ 'body.view' è€Œä¸æ˜¯ 'body.storage'
    params = {'cql': cql, 'limit': 1, 'expand': 'body.view'}
    
    response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    response.raise_for_status()
    results = response.json().get('results', [])
    if not results:
        print("âš ï¸ æ‰¾ä¸åˆ°é€±å ±")
        sys.exit(1)
    return results[0]

def extract_first_project_link(report_body):
    """å¾é€±å ± Rendered HTML ä¸­æŠ“å– Project æ¬„ä½çš„ç¬¬ä¸€å€‹é€£çµ"""
    soup = BeautifulSoup(report_body, 'html.parser')
    
    tables = soup.find_all('table')
    for table in tables:
        # å°‹æ‰¾è¡¨é ­
        headers = []
        # æœ‰äº›è¡¨æ ¼ç”¨ th, æœ‰äº›ç”¨ td class="highlight"
        header_row = table.find('tr')
        if not header_row: continue
        
        for cell in header_row.find_all(['th', 'td']):
            headers.append(cell.get_text().strip())
            
        if "Project" in headers:
            proj_idx = headers.index("Project")
            
            # æ‰¾ç¬¬ä¸€åˆ—æœ‰è³‡æ–™çš„ row
            rows = table.find_all('tr')
            for row in rows[1:]: # è·³éè¡¨é ­
                cols = row.find_all('td')
                if len(cols) > proj_idx:
                    # åœ¨ View æ¨¡å¼ä¸‹ï¼Œé€£çµå°±æ˜¯æ¨™æº–çš„ <a href="...">
                    link_tag = cols[proj_idx].find('a')
                    
                    if link_tag:
                        # å„ªå…ˆå˜—è©¦æŠ“å– Page ID (æœ€æº–ç¢º)
                        # Confluence View é€£çµé€šå¸¸å¸¶æœ‰ data-linked-resource-id
                        page_id = link_tag.get('data-linked-resource-id')
                        
                        if page_id:
                            print(f"ğŸ¯ é–å®šç›®æ¨™å°ˆæ¡ˆ (ID: {page_id})")
                            return {'id': page_id}
                        
                        # å¦‚æœæ²’æœ‰ IDï¼ŒæŠ“æ–‡å­—æ¨™é¡Œ
                        title = link_tag.get_text().strip()
                        if title:
                            print(f"ğŸ¯ é–å®šç›®æ¨™å°ˆæ¡ˆ (Title: {title})")
                            return {'title': title}

    print("âš ï¸ åœ¨é€±å ±ä¸­æ‰¾ä¸åˆ°ä»»ä½• Project é€£çµ (è«‹ç¢ºèªè¡¨æ ¼æ¨™é¡Œæ˜¯å¦ç‚º 'Project')")
    return None

def get_page_by_id(page_id):
    """é€é ID å–å¾—é é¢è³‡è¨Š (Storage æ ¼å¼ï¼Œç‚ºäº†ç·¨è¼¯)"""
    url = f"{API_ENDPOINT}/{page_id}"
    params = {'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    if resp.status_code == 200:
        return resp.json()
    return None

def get_page_by_title(title):
    """é€éæ¨™é¡Œå–å¾—é é¢è³‡è¨Š (Storage æ ¼å¼ï¼Œç‚ºäº†ç·¨è¼¯)"""
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    results = resp.json().get('results', [])
    if results:
        return results[0]
    return None

def is_red_row(tr):
    """åˆ¤æ–·é€™ä¸€è¡Œæ˜¯å¦æœ‰ç´…å­—"""
    # æª¢æŸ¥ style å±¬æ€§ä¸­çš„é¡è‰²è¨­å®š
    tags_with_style = tr.find_all(lambda tag: tag.has_attr('style'))
    for tag in tags_with_style:
        style = tag['style'].lower()
        if 'rgb(255, 0, 0)' in style or '#ff0000' in style:
            return True
    
    # ä¹Ÿæœ‰å¯èƒ½æ˜¯åœ¨ <font color="red"> (èˆŠç‰ˆ)
    if tr.find('font', color="red") or tr.find('font', color="#ff0000"):
        return True
        
    return False

def clean_project_page_content(html_content):
    """æ ¸å¿ƒé‚è¼¯ï¼šç˜¦èº« + æ­¸æª”"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. ç¢ºä¿æœ‰ History å€å¡Š
    history_header = soup.find(lambda tag: tag.name in ['h1', 'h2'] and 'History' in tag.get_text())
    
    if not history_header:
        print("   â„¹ï¸ æ‰¾ä¸åˆ° History å€å¡Šï¼Œæ­£åœ¨å»ºç«‹...")
        history_header = soup.new_tag('h1')
        history_header.string = "History"
        soup.append(history_header)
    
    all_headers = soup.find_all(['h3', 'h4']) 
    
    changed = False
    
    for header in all_headers:
        # æª¢æŸ¥é€™å€‹æ¨™é¡Œæ˜¯å¦åœ¨ History ä¹‹å¾Œ (å¦‚æœæ˜¯ï¼Œå‰‡ä¸è™•ç†)
        if history_header and header.sourceline and history_header.sourceline:
             if header.sourceline > history_header.sourceline:
                continue
        # å‚™ç”¨ï¼šå¦‚æœ sourceline æ²’æŠ“åˆ°ï¼Œç”¨éæ­·æ³•åˆ¤æ–· (ç•¥)
            
        header_text = header.get_text().strip()
        if header_text.lower() in ['history', 'work item table']:
            continue
            
        # æ‰¾é€™å€‹æ¨™é¡Œç·Šæ¥è‘—çš„è¡¨æ ¼
        next_node = header.find_next_sibling()
        target_table = None
        while next_node:
            if next_node.name == 'table':
                target_table = next_node
                break
            if next_node.name in ['h1', 'h2', 'h3', 'h4']: 
                break
            next_node = next_node.find_next_sibling()
            
        if not target_table:
            continue
            
        print(f"   ğŸ” æª¢æŸ¥é …ç›®: {header_text}")
        
        tbody = target_table.find('tbody')
        if not tbody: continue
        
        rows = tbody.find_all('tr')
        if not rows: continue
        
        # ç¬¬ä¸€åˆ—é€šå¸¸æ˜¯è¡¨é ­
        data_rows = rows[1:] 
        
        keep_rows = []
        archive_rows = []
        
        count = 0
        for row in data_rows:
            # è¦å‰‡ B: ç´…å­—çµ•å°ä¿ç•™
            if is_red_row(row):
                keep_rows.append(row)
                print("      ğŸ”´ ç™¼ç¾ç´…å­—ï¼Œå¼·åˆ¶ä¿ç•™")
                continue
            
            # è¦å‰‡ A: ä¿ç•™å‰ N ç­†
            if count < KEEP_LIMIT:
                keep_rows.append(row)
                count += 1
            else:
                # è¦å‰‡ C: å…¶é¤˜æ­¸æª”
                archive_rows.append(row)
        
        if archive_rows:
            print(f"      âœ‚ï¸ éœ€æ­¸æª” {len(archive_rows)} ç­†è³‡æ–™...")
            changed = True
            
            # 3.1 å¾ä¸»è¡¨æ ¼ç§»é™¤
            for row in archive_rows:
                row.extract()
                
            # 3.2 æ”¾å…¥ History
            hist_item_header = None
            curr = history_header.next_sibling
            while curr:
                if curr.name in ['h3', 'h4'] and curr.get_text().strip() == header_text:
                    hist_item_header = curr
                    break
                curr = curr.next_sibling
            
            hist_table = None
            if hist_item_header:
                curr = hist_item_header.next_sibling
                while curr:
                    if curr.name == 'table':
                        hist_table = curr
                        break
                    if curr.name in ['h1', 'h2', 'h3', 'h4']: break
                    curr = curr.next_sibling
            else:
                print(f"      ğŸ†• History ä¸­ç„¡ [{header_text}]ï¼Œæ­£åœ¨æ–°å»º...")
                new_h4 = soup.new_tag(header.name)
                new_h4.string = header_text
                soup.append(new_h4)
                
                hist_table = soup.new_tag('table')
                orig_thead = rows[0]
                import copy
                new_thead = copy.copy(orig_thead) 
                hist_table.append(new_thead)
                soup.append(hist_table)
            
            if not hist_table.find('tbody'):
                hist_table.append(soup.new_tag('tbody'))
                
            for row in archive_rows:
                hist_table.append(row)
                
    return str(soup) if changed else None

def update_page(page_data, new_content):
    """å›å­˜é é¢ï¼Œä½¿ç”¨éœé»˜æ›´æ–°"""
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜é é¢: {page_data['title']} (éœé»˜æ¨¡å¼)...")
    
    url = f"{API_ENDPOINT}/{page_data['id']}"
    
    payload = {
        "version": {"number": page_data['version']['number'] + 1, "minorEdit": True},
        "title": page_data['title'],
        "type": "page",
        "body": {
            "storage": {
                "value": new_content,
                "representation": "storage"
            }
        }
    }
    
    resp = requests.put(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), 
                       headers=get_headers(), data=json.dumps(payload))
    resp.raise_for_status()
    print("âœ… æ›´æ–°æˆåŠŸï¼")

def main():
    print("=== Confluence å°ˆæ¡ˆé é¢æ•´ç†æ©Ÿå™¨äºº (Test Mode: Only 1st Link) ===")
    
    # 1. æ‰¾é€±å ± (View æ ¼å¼)
    report = find_latest_report()
    
    # 2. æŠ“ç¬¬ä¸€å€‹å°ˆæ¡ˆé€£çµ
    target_info = extract_first_project_link(report['body']['view']['value'])
    
    if not target_info:
        print("çµæŸï¼šæ²’æœ‰æ‰¾åˆ°å¯è™•ç†çš„å°ˆæ¡ˆé€£çµã€‚")
        return

    # 3. è®€å–è©²å°ˆæ¡ˆé é¢ (Storage æ ¼å¼)
    if 'id' in target_info:
        page_data = get_page_by_id(target_info['id'])
    else:
        page_data = get_page_by_title(target_info['title'])
        
    if not page_data:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•æ‰¾åˆ°é é¢")
        return
        
    print(f"ğŸ“– è®€å–é é¢å…§å®¹: {page_data['title']} (ID: {page_data['id']})")
    
    # 4. åŸ·è¡Œæ¸…ç†é‚è¼¯
    new_content = clean_project_page_content(page_data['body']['storage']['value'])
    
    # 5. å›å­˜
    if new_content:
        update_page(page_data, new_content)
    else:
        print("ğŸ‘Œ é é¢ç„¡éœ€è®Šæ›´")

if __name__ == "__main__":
    main()
