import os
import requests
import json
import re
import sys
from datetime import datetime
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse
from bs4 import BeautifulSoup, Tag

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
KEEP_LIMIT = 5  # ä¿ç•™æœ€æ–°çš„å¹¾ç­†è³‡æ–™

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

parsed = urlparse(RAW_URL)
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

def find_latest_report():
    """æ‰¾åˆ°æœ€æ–°çš„é€±å ±ï¼Œç”¨ä¾†æŠ“å– Project åˆ—è¡¨"""
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    url = f"{API_ENDPOINT}/search"
    params = {'cql': cql, 'limit': 1, 'expand': 'body.storage'}
    
    response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    response.raise_for_status()
    results = response.json().get('results', [])
    if not results:
        print("âš ï¸ æ‰¾ä¸åˆ°é€±å ±")
        sys.exit(1)
    return results[0]

def extract_first_project_link(report_body):
    """å¾é€±å ± HTML ä¸­æŠ“å– Project æ¬„ä½çš„ç¬¬ä¸€å€‹é€£çµ"""
    soup = BeautifulSoup(report_body, 'html.parser')
    
    # å‡è¨­ Project åˆ—è¡¨åœ¨ç¬¬ä¸€å€‹è¡¨æ ¼ä¸­
    # é€™è£¡æˆ‘å€‘å°‹æ‰¾åŒ…å« "Project" è¡¨é ­çš„è¡¨æ ¼
    target_link = None
    
    tables = soup.find_all('table')
    for table in tables:
        headers = [th.get_text().strip() for th in table.find_all('th')]
        if "Project" in headers:
            # æ‰¾åˆ° Project æ¬„ä½æ˜¯ç¬¬å¹¾å€‹ (index)
            proj_idx = headers.index("Project")
            
            # æ‰¾ç¬¬ä¸€åˆ—æœ‰è³‡æ–™çš„ row
            rows = table.find_all('tr')
            for row in rows[1:]: # è·³éè¡¨é ­
                cols = row.find_all('td')
                if len(cols) > proj_idx:
                    link_tag = cols[proj_idx].find('a')
                    if link_tag:
                        # æŠ“å– pageId (é€šå¸¸é€£çµæ˜¯ /wiki/pages/viewpage.action?pageId=xxxx)
                        # æˆ–è€… storage format æ˜¯ <ac:link><ri:page ri:content-title="WeeklyStatus_BUSGW" /></ac:link>
                        # BeautifulSoup è§£æ Storage Format çš„ ri:page
                        ri_page = link_tag.find('ri:page')
                        if ri_page and ri_page.get('ri:content-title'):
                            target_title = ri_page.get('ri:content-title')
                            print(f"ğŸ¯ é–å®šç›®æ¨™å°ˆæ¡ˆé é¢: {target_title}")
                            return target_title
                        
                        # å‚™ç”¨ï¼šå¦‚æœæ˜¯å‚³çµ± href
                        href = link_tag.get('href')
                        if href and "pageId=" in href:
                            # é€™ç¨®æƒ…æ³æ¯”è¼ƒå°‘è¦‹æ–¼ Storage Formatï¼Œä½†é é˜²è¬ä¸€
                            pass
                            
    print("âš ï¸ åœ¨é€±å ±ä¸­æ‰¾ä¸åˆ°ä»»ä½• Project é€£çµ")
    return None

def get_page_by_title(title):
    """é€éæ¨™é¡Œå–å¾—é é¢è³‡è¨Š"""
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version'}
    resp = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
    results = resp.json().get('results', [])
    if results:
        return results[0]
    return None

def is_red_row(tr):
    """åˆ¤æ–·é€™ä¸€è¡Œæ˜¯å¦æœ‰ç´…å­—"""
    # æª¢æŸ¥ style å±¬æ€§ä¸­çš„é¡è‰²è¨­å®š
    # Confluence ç´…å­—é€šå¸¸æ˜¯ color: rgb(255, 0, 0); æˆ– #ff0000
    tags_with_style = tr.find_all(lambda tag: tag.has_attr('style'))
    for tag in tags_with_style:
        style = tag['style'].lower()
        if 'rgb(255, 0, 0)' in style or '#ff0000' in style:
            return True
    
    # ä¹Ÿæœ‰å¯èƒ½æ˜¯åœ¨ <font color="red"> (èˆŠç‰ˆ)
    if tr.find('font', color="red") or tr.find('font', color="#ff0000"):
        return True
        
    return False

def clean_project_page_content(html_content):
    """æ ¸å¿ƒé‚è¼¯ï¼šç˜¦èº« + æ­¸æª”"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. ç¢ºä¿æœ‰ History å€å¡Š
    history_header = soup.find(lambda tag: tag.name in ['h1', 'h2'] and 'History' in tag.get_text())
    history_container = None
    
    if not history_header:
        print("   â„¹ï¸ æ‰¾ä¸åˆ° History å€å¡Šï¼Œæ­£åœ¨å»ºç«‹...")
        history_header = soup.new_tag('h1')
        history_header.string = "History"
        soup.append(history_header)
        # History ä¹‹å¾Œçš„å…§å®¹éƒ½ç®— History å€
    
    # 2. å°‹æ‰¾æ‰€æœ‰ä¸»è¦é …ç›® (Item) çš„å€å¡Š
    # é‚è¼¯ï¼šé€šå¸¸æ˜¯ <h4>æ¨™é¡Œ</h4> æ¥è‘—ä¸€å€‹ <table>
    # æˆ‘å€‘åªè™•ç† History ä¹‹å‰çš„è¡¨æ ¼
    
    # ç‚ºäº†é¿å…æŠ“åˆ° History è£¡é¢çš„è¡¨æ ¼ï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹åœæ­¢é»
    # ç°¡å–®ä½œæ³•ï¼šéæ­·æ‰€æœ‰ h4ï¼Œå¦‚æœè©² h4 åœ¨ history_header ä¹‹å¾Œï¼Œå°±å¿½ç•¥
    
    all_headers = soup.find_all(['h3', 'h4']) # å‡è¨­é …ç›®æ¨™é¡Œæ˜¯ h3 æˆ– h4
    
    changed = False
    
    for header in all_headers:
        # æª¢æŸ¥é€™å€‹æ¨™é¡Œæ˜¯å¦åœ¨ History ä¹‹å¾Œ (å¦‚æœæ˜¯ï¼Œå‰‡ä¸è™•ç†ï¼Œå› ç‚ºé‚£æ˜¯æ­¸æª”å€)
        if history_header and header.sourceline > history_header.sourceline:
            continue
            
        header_text = header.get_text().strip()
        # æ’é™¤ä¸€äº›éé …ç›®çš„æ¨™é¡Œ
        if header_text.lower() in ['history', 'work item table']:
            continue
            
        # æ‰¾é€™å€‹æ¨™é¡Œç·Šæ¥è‘—çš„è¡¨æ ¼
        next_node = header.find_next_sibling()
        target_table = None
        while next_node:
            if next_node.name == 'table':
                target_table = next_node
                break
            if next_node.name in ['h1', 'h2', 'h3', 'h4']: # é‡åˆ°ä¸‹ä¸€å€‹æ¨™é¡Œå°±åœ
                break
            next_node = next_node.find_next_sibling()
            
        if not target_table:
            continue
            
        print(f"   ğŸ” æª¢æŸ¥é …ç›®: {header_text}")
        
        # 3. è™•ç†è¡¨æ ¼è¡Œ
        tbody = target_table.find('tbody')
        if not tbody: continue
        
        rows = tbody.find_all('tr')
        if not rows: continue
        
        # ç¬¬ä¸€åˆ—é€šå¸¸æ˜¯è¡¨é ­ (Item, Update)ï¼Œè·³é
        data_rows = rows[1:] 
        
        keep_rows = []
        archive_rows = []
        
        count = 0
        for row in data_rows:
            # è¦å‰‡ B: ç´…å­—çµ•å°ä¿ç•™
            if is_red_row(row):
                keep_rows.append(row)
                # ç´…å­—ä¸ä½”ç”¨è¨ˆæ•¸åé¡ (æ ¹æ“šæ‚¨çš„éœ€æ±‚ï¼šç´…å­—æ˜¯ä¾‹å¤–)
                print("      ğŸ”´ ç™¼ç¾ç´…å­—ï¼Œå¼·åˆ¶ä¿ç•™")
                continue
            
            # è¦å‰‡ A: ä¿ç•™å‰ N ç­†
            if count < KEEP_LIMIT:
                keep_rows.append(row)
                count += 1
            else:
                # è¦å‰‡ C: å…¶é¤˜æ­¸æª”
                archive_rows.append(row)
        
        if archive_rows:
            print(f"      âœ‚ï¸ éœ€æ­¸æª” {len(archive_rows)} ç­†è³‡æ–™...")
            changed = True
            
            # 3.1 å¾ä¸»è¡¨æ ¼ç§»é™¤é€™äº›è¡Œ
            for row in archive_rows:
                row.extract() # å¾ HTML æ¨¹ä¸­æ‹”é™¤
                
            # 3.2 æ”¾å…¥ History
            # æ‰¾ History å€å¡Šä¸‹æ˜¯å¦å·²ç¶“æœ‰é€™å€‹æ¨™é¡Œçš„è¡¨æ ¼
            # é€™æ¯”è¼ƒé›£å®šä½ï¼Œæˆ‘å€‘æ¡ç”¨ç°¡å–®ç­–ç•¥ï¼š
            # åœ¨ History Header ä¹‹å¾Œæ‰¾åŒåçš„ h3/h4
            
            hist_item_header = None
            # æœå°‹ history_header ä¹‹å¾Œçš„æ‰€æœ‰å…„å¼Ÿç¯€é»
            curr = history_header.next_sibling
            while curr:
                if curr.name in ['h3', 'h4'] and curr.get_text().strip() == header_text:
                    hist_item_header = curr
                    break
                curr = curr.next_sibling
            
            hist_table = None
            if hist_item_header:
                # æ‰¾åˆ°äº†ï¼Œæ‰¾å®ƒä¸‹é¢çš„è¡¨æ ¼
                curr = hist_item_header.next_sibling
                while curr:
                    if curr.name == 'table':
                        hist_table = curr
                        break
                    if curr.name in ['h1', 'h2', 'h3', 'h4']: break
                    curr = curr.next_sibling
            else:
                # æ²’æ‰¾åˆ°ï¼Œæ–°å»ºæ¨™é¡Œå’Œè¡¨æ ¼
                print(f"      ğŸ†• History ä¸­ç„¡ [{header_text}]ï¼Œæ­£åœ¨æ–°å»º...")
                new_h4 = soup.new_tag(header.name) # ä½¿ç”¨è·ŸåŸæœ¬ä¸€æ¨£çš„å±¤ç´š (h3/h4)
                new_h4.string = header_text
                soup.append(new_h4)
                
                hist_table = soup.new_tag('table')
                # è¤‡è£½åŸè¡¨æ ¼çš„è¡¨é ­
                orig_thead = rows[0] # åŸæœ¬çš„ç¬¬ä¸€åˆ—
                # æ³¨æ„ï¼šé€™è£¡è¦æ·±æ‹·è²è¡¨é ­ï¼Œä¸ç„¶æœƒè¢«æ‹”èµ°
                import copy
                new_thead = copy.copy(orig_thead) 
                hist_table.append(new_thead)
                soup.append(hist_table)
            
            # 3.3 è²¼ä¸Šè³‡æ–™
            # ç¢ºä¿ hist_table æœ‰ tbody (BeautifulSoup æœ‰æ™‚ä¸æœƒè‡ªå‹•å»º)
            if not hist_table.find('tbody'):
                hist_table.append(soup.new_tag('tbody'))
                
            # å¦‚æœæ˜¯æ–°å»ºçš„è¡¨æ ¼ï¼Œç¬¬ä¸€è¡Œè¦æ˜¯è¡¨é ­
            # é€™è£¡ç°¡å–®è™•ç†ï¼šç›´æ¥ append row
            for row in archive_rows:
                hist_table.append(row)
                
    return str(soup) if changed else None

def update_page(page_data, new_content):
    """å›å­˜é é¢ï¼Œä½¿ç”¨éœé»˜æ›´æ–°"""
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜é é¢: {page_data['title']} (éœé»˜æ¨¡å¼)...")
    
    url = f"{API_ENDPOINT}/{page_data['id']}"
    
    payload = {
        "version": {"number": page_data['version']['number'] + 1, "minorEdit": True}, # minorEdit = ä¸é€šçŸ¥
        "title": page_data['title'],
        "type": "page",
        "body": {
            "storage": {
                "value": new_content,
                "representation": "storage"
            }
        }
    }
    
    resp = requests.put(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), 
                       headers=get_headers(), data=json.dumps(payload))
    resp.raise_for_status()
    print("âœ… æ›´æ–°æˆåŠŸï¼")

def main():
    print("=== Confluence å°ˆæ¡ˆé é¢æ•´ç†æ©Ÿå™¨äºº (Test Mode: Only 1st Link) ===")
    
    # 1. æ‰¾é€±å ±
    report = find_latest_report()
    
    # 2. æŠ“ç¬¬ä¸€å€‹å°ˆæ¡ˆé€£çµ
    target_title = extract_first_project_link(report['body']['storage']['value'])
    
    if not target_title:
        print("çµæŸï¼šæ²’æœ‰æ‰¾åˆ°å¯è™•ç†çš„å°ˆæ¡ˆé€£çµã€‚")
        return

    # 3. è®€å–è©²å°ˆæ¡ˆé é¢
    page_data = get_page_by_title(target_title)
    if not page_data:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•é€éæ¨™é¡Œ '{target_title}' æ‰¾åˆ°é é¢ ID")
        return
        
    print(f"ğŸ“– è®€å–é é¢å…§å®¹: {target_title} (ID: {page_data['id']})")
    
    # 4. åŸ·è¡Œæ¸…ç†é‚è¼¯
    new_content = clean_project_page_content(page_data['body']['storage']['value'])
    
    # 5. å›å­˜ (å¦‚æœæœ‰è®Šæ›´)
    if new_content:
        update_page(page_data, new_content)
    else:
        print("ğŸ‘Œ é é¢ç„¡éœ€è®Šæ›´ (æ²’æœ‰è¶…éé™åˆ¶çš„èˆŠè³‡æ–™ï¼Œæˆ–å…¨éƒ¨éƒ½æ˜¯ç´…å­—)")

if __name__ == "__main__":
    main()
