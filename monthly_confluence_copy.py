import os
import requests
import json
import re
import sys
import html
from datetime import datetime
from dateutil.relativedelta import relativedelta
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse

# ==========================================
# 1. è¨­å®šå€
# ==========================================
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")
PARENT_PAGE_TITLE = "Personal Tasks"

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("âŒ éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸ (CONF_URL, CONF_USER, CONF_PASS)")
    sys.exit(1)

parsed_url = urlparse(RAW_URL)
BASE_URL = f"{parsed_url.scheme}://{parsed_url.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½
# ==========================================

def get_page_by_id(page_id):
    """é€é ID å–å¾—å®Œæ•´é é¢å…§å®¹"""
    url = f"{API_ENDPOINT}/{page_id}"
    params = {'expand': 'body.storage,version,ancestors,space'}
    try:
        r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        print(f"âŒ è®€å–é é¢å¤±æ•— (ID: {page_id}): {e}")
    return None

def get_page_id_by_title(title):
    """é€éæ¨™é¡Œæœå°‹ ID"""
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version,ancestors'}
    try:
        r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        r.raise_for_status()
        results = r.json().get('results', [])
        if results: return results[0]
    except Exception as e:
        print(f"âŒ æœå°‹é é¢ '{title}' å¤±æ•—: {e}")
    return None

def get_child_pages(parent_id):
    """å–å¾—æ‰€æœ‰å­é é¢"""
    url = f"{API_ENDPOINT}/{parent_id}/child/page"
    params = {'limit': 100, 'expand': 'version'} 
    try:
        r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        r.raise_for_status()
        return r.json().get('results', [])
    except Exception as e:
        print(f"âŒ å–å¾—å­é é¢å¤±æ•—: {e}")
        return []

def find_latest_monthly_page():
    print(f"ğŸ” æ­£åœ¨æœå°‹çˆ¶é é¢: {PARENT_PAGE_TITLE}...")
    parent_page = get_page_id_by_title(PARENT_PAGE_TITLE)
    if not parent_page:
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¶é é¢: {PARENT_PAGE_TITLE}")
        sys.exit(1)

    parent_id = parent_page['id']
    print(f"âœ… æ‰¾åˆ°çˆ¶é é¢ ID: {parent_id}")

    children = get_child_pages(parent_id)
    monthly_pages = []
    
    for child in children:
        title = child['title']
        if re.match(r'^\d{6}$', title):
            monthly_pages.append(child)
    
    if not monthly_pages:
        print("âš ï¸ åœ¨ Personal Tasks ä¸‹æ‰¾ä¸åˆ°ä»»ä½• YYYYMM æ ¼å¼çš„é é¢ã€‚")
        sys.exit(1)

    monthly_pages.sort(key=lambda x: x['title'], reverse=True)
    latest_basic_info = monthly_pages[0]
    
    print(f"ğŸ“… æ‰¾åˆ°æœ€æ–°æœˆä»½æ¨™é¡Œ: {latest_basic_info['title']} (ID: {latest_basic_info['id']})")
    
    # ä½¿ç”¨ ID ç²å–å®Œæ•´å…§å®¹
    full_page = get_page_by_id(latest_basic_info['id'])
    return full_page

# ------------------------------------------
# æ—¥æœŸè™•ç†é‚è¼¯ (YYYY-MM-DD)
# ------------------------------------------
def increment_date_match(match):
    full_date = match.group(0)
    sep = match.group(2)
    try:
        fmt = f"%Y{sep}%m{sep}%d"
        dt = datetime.strptime(full_date, fmt)
        new_dt = dt + relativedelta(months=1)
        return new_dt.strftime(fmt)
    except ValueError:
        return full_date

# ------------------------------------------
# ã€æ–°å¢ã€‘NPI æ¨™ç±¤è™•ç†é‚è¼¯ (NPI_YYYYMM)
# ------------------------------------------
def increment_npi_match(match):
    prefix = match.group(1) # "NPI_"
    date_str = match.group(2) # "202512"
    try:
        # è§£æ YYYYMM
        dt = datetime.strptime(date_str, "%Y%m")
        # åŠ ä¸€å€‹æœˆ
        new_dt = dt + relativedelta(months=1)
        # æ ¼å¼åŒ–å› YYYYMM
        new_date_str = new_dt.strftime("%Y%m")
        
        result = f"{prefix}{new_date_str}"
        # print(f"      ğŸ‘‰ NPIæ›´æ–°: {match.group(0)} -> {result}")
        return result
    except ValueError:
        return match.group(0)

def process_content_all(html_content):
    """
    åŸ·è¡Œæ‰€æœ‰çš„å…§å®¹æ›¿æ›é‚è¼¯ï¼š
    1. æ—¥æœŸæ ¼å¼ (2025-12-01)
    2. NPI æ¨™ç±¤ (NPI_202512)
    """
    print("ğŸ”§ æ­£åœ¨è™•ç†å…§å®¹ (åŒ…å«æ—¥æœŸèˆ‡ NPI æ¨™ç±¤)...")
    
    # --- 1. è™•ç†æ¨™æº–æ—¥æœŸ (YYYY-MM-DD æˆ– YYYY/MM/DD) ---
    date_pattern = re.compile(r'(\d{4})([-/.])(\d{1,2})\2(\d{1,2})')
    content_v1, count_date = date_pattern.subn(increment_date_match, html_content)
    
    # --- 2. è™•ç† NPI æ¨™ç±¤ (NPI_YYYYMM) ---
    # Regex èªªæ˜: (NPI_) æ¥ 6ä½æ•¸å­—
    npi_pattern = re.compile(r'(NPI_)(\d{6})')
    content_final, count_npi = npi_pattern.subn(increment_npi_match, content_v1)
    
    print(f"ğŸ“Š è™•ç†å ±å‘Š:")
    print(f"   - ä¿®æ”¹äº† {count_date} å€‹æ¨™æº–æ—¥æœŸ (å¦‚ 2025-12-01)")
    print(f"   - ä¿®æ”¹äº† {count_npi} å€‹ NPI æ¨™ç±¤ (å¦‚ NPI_202512)")
    
    if count_date == 0 and count_npi == 0:
        print("âš ï¸ è­¦å‘Šï¼šæ²’æœ‰ç™¼ç¾ä»»ä½•éœ€ä¿®æ”¹çš„æ—¥æœŸæˆ–æ¨™ç±¤ã€‚")
    
    return content_final

def create_new_month_page(latest_page):
    current_title = latest_page['title']
    try:
        current_date_obj = datetime.strptime(current_title, "%Y%m")
        next_date_obj = current_date_obj + relativedelta(months=1)
        next_title = next_date_obj.strftime("%Y%m")
    except ValueError:
        print("âŒ æ¨™é¡Œæ—¥æœŸæ ¼å¼éŒ¯èª¤")
        sys.exit(1)

    print(f"ğŸš€ æº–å‚™å»ºç«‹æ–°é é¢: {next_title}")

    if get_page_id_by_title(next_title):
        print(f"âš ï¸ è·³éï¼šé é¢ '{next_title}' å·²ç¶“å­˜åœ¨ï¼")
        return

    # å–å¾—åŸå§‹å…§å®¹
    original_body = latest_page['body']['storage']['value']
    
    # åŸ·è¡Œæ‰€æœ‰æ›¿æ›
    new_body = process_content_all(original_body)

    # å–å¾—çˆ¶å±¤ ID
    if latest_page.get('ancestors'):
        parent_id = latest_page['ancestors'][-1]['id']
    else:
        p_page = get_page_id_by_title(PARENT_PAGE_TITLE)
        parent_id = p_page['id']

    payload = {
        "type": "page",
        "title": next_title,
        "ancestors": [{"id": parent_id}],
        "space": {"key": latest_page['space']['key']},
        "body": {
            "storage": {
                "value": new_body,
                "representation": "storage"
            }
        },
        # ã€ä¸é€šçŸ¥è¿½è¹¤è€…è¨­å®šã€‘
        "version": {
            "number": 1,
            "minorEdit": True
        },
        "status": "current"
    }

    try:
        response = requests.post(
            API_ENDPOINT, 
            auth=HTTPBasicAuth(USERNAME, API_TOKEN),
            headers=get_headers(),
            data=json.dumps(payload)
        )
        response.raise_for_status()
        
        data = response.json()
        base_url = BASE_URL.rstrip('/')
        link_suffix = data['_links']['webui']
        full_link = f"{base_url}/wiki{link_suffix}" if not link_suffix.startswith('/wiki') else f"{base_url}{link_suffix}"
        
        print(f"ğŸ‰ æˆåŠŸå»ºç«‹ï¼é€£çµ: {full_link}")

    except requests.exceptions.HTTPError as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        print(f"éŒ¯èª¤å›æ‡‰: {response.text}")
        sys.exit(1)

def main():
    print(f"=== Confluence æœˆåº¦ JQL æ›´æ–°æ©Ÿå™¨äºº (v6.0 NPIæ”¯æ´ç‰ˆ) ===")
    try:
        latest_page = find_latest_monthly_page()
        if latest_page:
            create_new_month_page(latest_page)
        else:
            print("âŒ ç„¡æ³•å–å¾—ä¾†æºé é¢è³‡æ–™")
    except Exception as e:
        print(f"åŸ·è¡Œä¸­æ–·: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
