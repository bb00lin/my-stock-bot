import os
import requests
import json
import re
import sys
from datetime import datetime
from dateutil.relativedelta import relativedelta
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse
from bs4 import BeautifulSoup

# ==========================================
# 1. è¨­å®šå€ (ä½¿ç”¨èˆ‡åŸè…³æœ¬ç›¸åŒçš„ç’°å¢ƒè®Šæ•¸)
# ==========================================
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")

# çˆ¶é é¢æ¨™é¡Œï¼Œç”¨ä¾†å®šä½åŸºæº–é»
PARENT_PAGE_TITLE = "Personal Tasks"

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("âŒ éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸ (CONF_URL, CONF_USER, CONF_PASS)")
    sys.exit(1)

# è™•ç† URL
parsed_url = urlparse(RAW_URL)
BASE_URL = f"{parsed_url.scheme}://{parsed_url.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_headers():
    return {"Content-Type": "application/json"}

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼
# ==========================================

def get_page_id_by_title(title):
    """é€éæ¨™é¡Œæœå°‹é é¢ ID"""
    url = f"{API_ENDPOINT}"
    params = {'title': title, 'expand': 'body.storage,version,ancestors,space'}
    try:
        r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        r.raise_for_status()
        results = r.json().get('results', [])
        if results:
            return results[0]
    except Exception as e:
        print(f"âŒ æœå°‹é é¢ '{title}' å¤±æ•—: {e}")
    return None

def get_child_pages(parent_id):
    """å–å¾—æŸé é¢ä¸‹çš„æ‰€æœ‰å­é é¢"""
    url = f"{API_ENDPOINT}/{parent_id}/child/page"
    params = {'limit': 100, 'expand': 'version'} 
    try:
        r = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        r.raise_for_status()
        return r.json().get('results', [])
    except Exception as e:
        print(f"âŒ å–å¾—å­é é¢å¤±æ•—: {e}")
        return []

def find_latest_monthly_page():
    """
    1. æ‰¾åˆ° 'Personal Tasks'
    2. æ‰¾åˆ°åº•ä¸‹æ ¼å¼ç‚º YYYYMM çš„å­é é¢
    3. å›å‚³æœˆä»½æœ€å¤§çš„ä¸€å€‹
    """
    print(f"æ­£åœ¨æœå°‹çˆ¶é é¢: {PARENT_PAGE_TITLE}...")
    parent_page = get_page_id_by_title(PARENT_PAGE_TITLE)
    if not parent_page:
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¶é é¢: {PARENT_PAGE_TITLE}")
        sys.exit(1)

    parent_id = parent_page['id']
    print(f"âœ… æ‰¾åˆ°çˆ¶é é¢ ID: {parent_id}")

    children = get_child_pages(parent_id)
    monthly_pages = []
    
    for child in children:
        title = child['title']
        # æª¢æŸ¥æ˜¯å¦ç‚º 6 ä½æ•¸å­— (ä¾‹å¦‚ 202512)
        if re.match(r'^\d{6}$', title):
            monthly_pages.append(child)
    
    if not monthly_pages:
        print("âš ï¸ åœ¨ Personal Tasks ä¸‹æ‰¾ä¸åˆ°ä»»ä½• YYYYMM æ ¼å¼çš„é é¢ã€‚")
        sys.exit(1)

    # æ’åºæ‰¾åˆ°æœ€æ–°çš„æœˆä»½
    monthly_pages.sort(key=lambda x: x['title'], reverse=True)
    latest_page = monthly_pages[0]
    
    # é€™è£¡æˆ‘å€‘éœ€è¦é‡æ–°å–å¾—ä¸€æ¬¡ latest_page çš„è©³ç´°å…§å®¹ (åŒ…å« body.storage)ï¼Œå› ç‚º child API çµ¦çš„è³‡è¨Šè¼ƒå°‘
    full_latest_page = get_page_id_by_title(latest_page['title'])
    
    print(f"ğŸ“… æ‰¾åˆ°æœ€æ–°æœˆä»½é é¢: {full_latest_page['title']} (ID: {full_latest_page['id']})")
    return full_latest_page

def increment_date_in_text(text):
    """
    å°‡æ–‡å­—ä¸­çš„æ—¥æœŸ (YYYY-MM-DD æˆ– YYYY/MM/DD) åŠ  1 å€‹æœˆ
    """
    date_pattern = re.compile(r'(\d{4})([-/])(\d{1,2})([-/])(\d{1,2})')

    def replace_date(match):
        year, sep1, month, sep2, day = match.groups()
        try:
            current_date = datetime(int(year), int(month), int(day))
            new_date = current_date + relativedelta(months=1)
            # ä¿æŒåŸå§‹åˆ†éš”ç¬¦è™Ÿ
            return f"{new_date.year}{sep1}{new_date.month:02d}{sep2}{new_date.day:02d}"
        except ValueError:
            return match.group(0)

    return date_pattern.sub(replace_date, text)

def process_jql_content(html_content):
    """
    è§£æ HTMLï¼Œåªä¿®æ”¹ Jira Macro (JQL) ä¸­çš„æ—¥æœŸ
    """
    print("æ­£åœ¨è™•ç† JQL æ—¥æœŸéå¢...")
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ‰¾åˆ°æ‰€æœ‰ Jira Macro
    jira_macros = soup.find_all('ac:structured-macro', attrs={"ac:name": "jira"})
    modified_count = 0
    
    for macro in jira_macros:
        jql_param = macro.find('ac:parameter', attrs={"ac:name": "jql"})
        if jql_param and jql_param.string:
            original_jql = jql_param.string
            new_jql = increment_date_in_text(original_jql)
            
            if original_jql != new_jql:
                # æ³¨æ„ï¼šBeautifulSoup ä¿®æ”¹ string çš„æ–¹å¼
                jql_param.string.replace_with(new_jql)
                modified_count += 1
                # print(f"   Debug: {original_jql} -> {new_jql}")

    print(f"ğŸ“Š å…±ä¿®æ”¹äº† {modified_count} å€‹ JQL æ—¥æœŸ")
    return str(soup)

def create_new_month_page(latest_page):
    # 1. è¨ˆç®—æ–°æ¨™é¡Œ (æœˆä»½+1)
    current_title = latest_page['title']
    try:
        current_date_obj = datetime.strptime(current_title, "%Y%m")
        next_date_obj = current_date_obj + relativedelta(months=1)
        next_title = next_date_obj.strftime("%Y%m")
    except ValueError:
        print("âŒ æ¨™é¡Œæ—¥æœŸæ ¼å¼è§£æéŒ¯èª¤ï¼Œç„¡æ³•è¨ˆç®—ä¸‹å€‹æœˆã€‚")
        sys.exit(1)

    print(f"ğŸš€ ç›®æ¨™å»ºç«‹æ–°é é¢: {next_title}")

    # 2. æª¢æŸ¥é‡è¤‡
    if get_page_id_by_title(next_title):
        print(f"âš ï¸ è·³éï¼šé é¢ '{next_title}' å·²ç¶“å­˜åœ¨ï¼")
        return

    # 3. è™•ç†å…§å®¹
    original_body = latest_page['body']['storage']['value']
    new_body = process_jql_content(original_body)

    # 4. æº–å‚™ Payload
    # å–å¾— parent_id (Personal Tasks çš„ ID)
    # latest_page['ancestors'] åˆ—è¡¨çš„æœ€å¾Œä¸€å€‹é€šå¸¸æ˜¯ç›´æ¥çˆ¶å±¤
    if latest_page.get('ancestors'):
        parent_id = latest_page['ancestors'][-1]['id']
    else:
        # å¦‚æœå–ä¸åˆ° ancestorï¼Œé‡æ–°æŸ¥è©¢ Personal Tasks çš„ ID
        p_page = get_page_id_by_title(PARENT_PAGE_TITLE)
        parent_id = p_page['id']

    payload = {
        "type": "page",
        "title": next_title,
        "ancestors": [{"id": parent_id}],
        "space": {"key": latest_page['space']['key']},
        "body": {
            "storage": {
                "value": new_body,
                "representation": "storage"
            }
        },
        # ä¸é€šçŸ¥è¿½è¹¤è€…è¨­å®š
        "version": {
            "number": 1,
            "minorEdit": True 
        }
    }

    # 5. ç™¼é€è«‹æ±‚
    try:
        response = requests.post(
            API_ENDPOINT, 
            auth=HTTPBasicAuth(USERNAME, API_TOKEN),
            headers=get_headers(),
            data=json.dumps(payload)
        )
        response.raise_for_status()
        
        data = response.json()
        webui = data['_links']['webui']
        # è™•ç†é€£çµæ ¼å¼ (æœ‰æ™‚ API å›å‚³çš„ webui åŒ…å« base urlï¼Œæœ‰æ™‚ä¸å«)
        if webui.startswith('http'):
            link = webui
        else:
            link = f"{BASE_URL}/wiki{webui}" if not webui.startswith('/wiki') else f"{BASE_URL}{webui}"
        
        print(f"ğŸ‰ æˆåŠŸå»ºç«‹ï¼é€£çµ: {link}")

    except requests.exceptions.HTTPError as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        print(response.text)
        sys.exit(1)

def main():
    print(f"=== Confluence æœˆåº¦ä»»å‹™è‡ªå‹•åŒ– (v1.0) ===")
    try:
        # 1. æ‰¾åˆ°æœ€æ–°çš„æœˆä»½é é¢
        latest_page = find_latest_monthly_page()
        # 2. å»ºç«‹ä¸‹å€‹æœˆçš„é é¢ (å«å…§å®¹ä¿®æ”¹)
        create_new_month_page(latest_page)
    except Exception as e:
        print(f"åŸ·è¡Œä¸­æ–·: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
