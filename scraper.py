import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re

class PokemonCenterScraper:
    def __init__(self):
        self.base_url = "https://www.pokemoncenter-online.com"
        # æ›´åŠ å®Œæ•´çš„ç€è¦½å™¨å½è£
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ja-JP,ja;q=0.9,en-US;q=0.8",
            "Referer": "https://www.pokemoncenter-online.com/",
            "Upgrade-Insecure-Requests": "1"
        }
        self.all_data = []

    def get_product_ids(self, max_pages=1):
        product_ids = []
        for page in range(1, max_pages + 1):
            # ä½¿ç”¨æ–°è‘—å•†å“æ¸…å–®é é¢
            list_url = f"{self.base_url}/?main_page=product_list&page={page}&sort=new"
            print(f"æ­£åœ¨å˜—è©¦é€£ç·šè‡³åˆ—è¡¨é : {list_url}")
            
            try:
                response = requests.get(list_url, headers=self.headers, timeout=15)
                print(f"ç¶²é å›æ‡‰ç‹€æ…‹ç¢¼: {response.status_code}")
                
                if response.status_code == 403:
                    print("ğŸš« éŒ¯èª¤ï¼šGitHub çš„ IP å·²è¢«å¯¶å¯å¤¢å®˜ç¶²å±è”½ (403 Forbidden)ã€‚")
                    print("å»ºè­°æ”¹åœ¨ã€å€‹äººé›»è…¦ã€ä¸ŠåŸ·è¡Œæ­¤è…³æœ¬ã€‚")
                    break
                
                soup = BeautifulSoup(response.text, 'html.parser')
                # åŒæ™‚æœå°‹å…©ç¨®é€£çµæ ¼å¼ï¼š
                # 1. ?p_cd=4521329...
                # 2. /4521329....html
                links = soup.find_all('a', href=True)
                for link in links:
                    href = link.get('href')
                    # æª¢æŸ¥æ˜¯å¦ç‚ºå•†å“é é€£çµ (é€šå¸¸æ˜¯ 13 ä½æ•¸å­—)
                    match = re.search(r'(\d{13})', href)
                    if match:
                        p_cd = match.group(1)
                        if p_cd not in product_ids:
                            product_ids.append(p_cd)
                
                print(f"ç¬¬ {page} é è§£æå®Œæˆï¼Œç›®å‰æ‰¾åˆ° {len(product_ids)} å€‹å•†å“ ID")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                print(f"é€£ç·šç™¼ç”ŸéŒ¯èª¤: {e}")
                
        return product_ids

    def get_product_details(self, p_cd):
        """æŠ“å–è©³ç´°è³‡è¨Šè¡¨æ ¼"""
        url = f"{self.base_url}/?p_cd={p_cd}"
        try:
            res = requests.get(url, headers=self.headers, timeout=15)
            if res.status_code != 200: return None
            
            soup = BeautifulSoup(res.text, 'html.parser')
            item = {
                "å•†å“åç¨±": soup.select_one('h1').get_text(strip=True) if soup.select_one('h1') else "N/A",
                "å•†å“ç·¨è™Ÿ": p_cd,
                "ç¶²å€": url
            }

            # è§£æè¦æ ¼è¡¨æ ¼
            table = soup.find('table', class_='common_table')
            if table:
                for row in table.find_all('tr'):
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        item[th.get_text(strip=True)] = td.get_text(strip=True)
            
            return item
        except:
            return None

    def start(self):
        ids = self.get_product_ids(max_pages=1) # å…ˆè©¦æŠ“ä¸€é 
        if not ids: return

        print(f"é–‹å§‹æŠ“å–è©³ç´°è³‡æ–™ï¼Œé è¨ˆæŠ“å– {len(ids)} ç­†...")
        for i, p_cd in enumerate(ids):
            data = self.get_product_details(p_cd)
            if data:
                self.all_data.append(data)
                print(f"[{i+1}/{len(ids)}] æˆåŠŸç²å–: {data['å•†å“åç¨±'][:15]}...")
            time.sleep(random.uniform(3, 5)) # é¿å…éå¿«è¢«æ“‹

        df = pd.DataFrame(self.all_data)
        df.to_excel("pokemon_data_fixed.xlsx", index=False)
        print("âœ… æŠ“å–æˆåŠŸï¼è«‹ä¸‹è¼‰ pokemon_data_fixed.xlsx")

if __name__ == "__main__":
    scraper = PokemonCenterScraper()
    scraper.start()
