import os
import requests
import json
import re
import sys
from datetime import date, timedelta
from requests.auth import HTTPBasicAuth

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸ (CONF_URL, CONF_USER, CONF_PASS)")
    sys.exit(1)

# --- æ™ºæ…§ç¶²å€ä¿®æ­£ (v2.0) ---
# ç¢ºä¿æˆ‘å€‘åªæ‹¿åˆ°æœ€ä¹¾æ·¨çš„åŸŸå (Domain)ï¼Œä¾‹å¦‚ https://qsiaiot.atlassian.net
# 1. ç§»é™¤çµå°¾æ–œç·š
BASE_URL = RAW_URL.rstrip('/')
# 2. å¦‚æœä½¿ç”¨è€…å¡«äº† /wiki çµå°¾ï¼ŒæŠŠå®ƒåˆ‡æ‰
if BASE_URL.endswith("/wiki"):
    BASE_URL = BASE_URL[:-5]

# ç¾åœ¨ BASE_URL ä¿è­‰æ˜¯ https://your-site.atlassian.net
print(f"API åŸºæº–ç¶²å€: {BASE_URL}")

def get_target_dates():
    """è¨ˆç®—æœ¬é€±æ—¥æœŸèˆ‡æª”å"""
    today = date.today()
    monday = today - timedelta(days=today.weekday())
    sunday = monday + timedelta(days=6)
    friday = monday + timedelta(days=4)
    return {
        "monday_str": monday.strftime("%Y-%m-%d"),
        "sunday_str": sunday.strftime("%Y-%m-%d"),
        "filename": friday.strftime("%Y%m%d")
    }

def find_latest_report():
    """æœå°‹æ¨™é¡Œç¬¦åˆ WeeklyReport_20... çš„æœ€æ–°é é¢"""
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport_20" ORDER BY created DESC'
    
    # æ­£ç¢ºçµ„å»º API è·¯å¾‘
    url = f"{BASE_URL}/wiki/rest/api/content/search"
    
    params = {
        'cql': cql,
        'limit': 1,
        'expand': 'body.storage,ancestors,space'
    }
    
    try:
        response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        
        # å¦‚æœæ˜¯ 404ï¼Œå°å‡ºæˆ‘å€‘åˆ°åº•æ‰“å»å“ªè£¡äº†ï¼Œæ–¹ä¾¿é™¤éŒ¯
        if response.status_code == 404:
            print(f"âŒ 404 éŒ¯èª¤ - è«‹æ±‚ç¶²å€: {response.url}")
            
        response.raise_for_status()
        results = response.json().get('results', [])
        
        if not results:
            print("âš ï¸ æœå°‹æˆåŠŸä½†ç„¡çµæœã€‚é€™å¯èƒ½æ˜¯å› ç‚ºï¼š")
            print("1. çœŸçš„æ²’æœ‰æ¨™é¡Œå« 'WeeklyReport_20' çš„é é¢ã€‚")
            print("2. API Token æ¬Šé™ä¸è¶³ä»¥çœ‹åˆ°è©²ç©ºé–“ã€‚")
            raise Exception("æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆ 'WeeklyReport_20' çš„é é¢")
        
        latest = results[0]
        print(f"âœ… æ‰¾åˆ°æœ€æ–°é€±å ±: {latest['title']} (ID: {latest['id']})")
        return latest
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ API è«‹æ±‚å¤±æ•—: {e}")
        if response.status_code == 401:
            print("ğŸ’¡ æç¤º: 401 é€šå¸¸ä»£è¡¨ API Token ç„¡æ•ˆæˆ– Email éŒ¯èª¤ã€‚")
        sys.exit(1)

def create_new_report(latest_page, dates):
    """åŸºæ–¼èˆŠå…§å®¹å»ºç«‹æ–°é é¢"""
    new_title = f"WeeklyReport_{dates['filename']}"
    print(f"æº–å‚™å»ºç«‹æ–°é é¢: {new_title}")
    
    original_body = latest_page['body']['storage']['value']
    
    # --- æ—¥æœŸæ›¿æ›é‚è¼¯ ---
    # å°‹æ‰¾æ‰€æœ‰ YYYY-MM-DD
    found_dates = re.findall(r"\d{4}-\d{1,2}-\d{1,2}", original_body)
    new_body = original_body
    
    if len(found_dates) >= 2:
        old_start = found_dates[0]
        old_end = found_dates[1]
        print(f"åµæ¸¬åˆ°èˆŠæ—¥æœŸå€é–“: {old_start} ~ {old_end}")
        
        # åªæ›¿æ›å‰å…©å€‹å‡ºç¾çš„æ—¥æœŸ (é¿å…èª¤å‚·å…§æ–‡)
        new_body = new_body.replace(old_start, dates['monday_str'], 1)
        new_body = new_body.replace(old_end, dates['sunday_str'], 1)
        print(f"å·²æ›¿æ›ç‚º: {dates['monday_str']} ~ {dates['sunday_str']}")
    else:
        print("âš ï¸ è­¦å‘Šï¼šèˆŠå…§å®¹ä¸­æ‰¾ä¸åˆ°è¶³å¤ çš„æ—¥æœŸæ ¼å¼ï¼Œå°‡ç›´æ¥è¤‡è£½å…§å®¹ã€‚")

    # --- æº–å‚™ Payload ---
    ancestors = []
    if latest_page.get('ancestors'):
        parent_id = latest_page['ancestors'][-1]['id']
        ancestors.append({'id': parent_id})
    
    space_key = latest_page['space']['key']
    
    payload = {
        "title": new_title,
        "type": "page",
        "space": {"key": space_key},
        "ancestors": ancestors,
        "body": {
            "storage": {
                "value": new_body,
                "representation": "storage"
            }
        }
    }
    
    create_url = f"{BASE_URL}/wiki/rest/api/content"
    headers = {"Content-Type": "application/json"}
    
    try:
        response = requests.post(
            create_url, 
            auth=HTTPBasicAuth(USERNAME, API_TOKEN),
            headers=headers,
            data=json.dumps(payload)
        )
        response.raise_for_status()
        new_page_data = response.json()
        
        # çµ„åˆç¶²å€ (è™•ç† webui å¯èƒ½æ²’æœ‰ /wiki é–‹é ­çš„æƒ…æ³)
        webui = new_page_data['_links']['webui']
        if not webui.startswith('/wiki'):
            webui = '/wiki' + webui
        full_link = f"{BASE_URL}{webui}"
        
        print(f"ğŸ‰ æˆåŠŸå»ºç«‹é é¢ï¼")
        print(f"é é¢ ID: {new_page_data['id']}")
        print(f"é€£çµ: {full_link}")
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        # å°å‡ºè©³ç´°éŒ¯èª¤è¨Šæ¯ (é€šå¸¸åŒ…å«ç‚ºä»€éº¼å¤±æ•—ï¼Œä¾‹å¦‚æ¨™é¡Œé‡è¤‡)
        print(f"ä¼ºæœå™¨å›æ‡‰: {response.text}")

def main():
    dates = get_target_dates()
    print(f"=== Confluence API è‡ªå‹•é€±å ±è…³æœ¬ (v2.0 URLä¿®æ­£ç‰ˆ) ===")
    print(f"ç›®æ¨™æ—¥æœŸ: {dates['monday_str']} ~ {dates['sunday_str']}")
    
    try:
        latest_page = find_latest_report()
        create_new_report(latest_page, dates)
    except Exception as e:
        print(f"åŸ·è¡Œä¸­æ–·: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
