import os
import requests
import json
import re
import sys
from datetime import date, timedelta
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸ (CONF_URL, CONF_USER, CONF_PASS)")
    sys.exit(1)

# --- ç¶²å€å¼·åŠ›æ·¨åŒ– (v3.0) ---
# å¼·åˆ¶è§£æå‡º scheme å’Œ netlocï¼Œæ¨æ£„æ‰€æœ‰å¾Œé¢çš„è·¯å¾‘
parsed = urlparse(RAW_URL)
# ç¢ºä¿æ˜¯ https://domain.atlassian.net é€™ç¨®æ ¼å¼
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"

print(f"åŸå§‹è¼¸å…¥ç¶²å€: {RAW_URL}")
print(f"æ·¨åŒ–å¾ŒåŸºæº–ç¶²å€: {BASE_URL}")

# Atlassian Cloud æ¨™æº– API è·¯å¾‘
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def get_target_dates():
    """è¨ˆç®—æœ¬é€±æ—¥æœŸèˆ‡æª”å"""
    today = date.today()
    monday = today - timedelta(days=today.weekday())
    sunday = monday + timedelta(days=6)
    friday = monday + timedelta(days=4)
    return {
        "monday_str": monday.strftime("%Y-%m-%d"),
        "sunday_str": sunday.strftime("%Y-%m-%d"),
        "filename": friday.strftime("%Y%m%d")
    }

def find_latest_report():
    """æœå°‹æ¨™é¡Œç¬¦åˆ WeeklyReport_20... çš„æœ€æ–°é é¢"""
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    cql = 'type=page AND title ~ "WeeklyReport_20" ORDER BY created DESC'
    
    url = f"{API_ENDPOINT}/search"
    
    params = {
        'cql': cql,
        'limit': 1,
        'expand': 'body.storage,ancestors,space'
    }
    
    try:
        response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        
        if response.status_code == 404:
            print(f"âŒ 404 éŒ¯èª¤ - è«‹æ±‚ç¶²å€: {response.url}")
            print("è«‹æª¢æŸ¥æ‚¨çš„ç¶²åŸŸæ˜¯å¦æ­£ç¢ºï¼Œæˆ–è€…è©²ç«™é»æ˜¯å¦ç‚º Cloud ç‰ˆæœ¬ã€‚")
            sys.exit(1)
            
        response.raise_for_status()
        results = response.json().get('results', [])
        
        if not results:
            print("âš ï¸ æœå°‹æˆåŠŸä½†ç„¡çµæœã€‚")
            print("ç³»çµ±æ‰¾ä¸åˆ°ä»»ä½•æ¨™é¡ŒåŒ…å« 'WeeklyReport_20' çš„é é¢ã€‚")
            sys.exit(1)
        
        latest = results[0]
        print(f"âœ… æ‰¾åˆ°æœ€æ–°é€±å ±: {latest['title']} (ID: {latest['id']})")
        return latest
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ API è«‹æ±‚å¤±æ•—: {e}")
        if response.status_code == 401:
            print("ğŸ’¡ æç¤º: 401 ä»£è¡¨ API Token ç„¡æ•ˆæˆ– Email éŒ¯èª¤ã€‚")
            print("è«‹ç¢ºèªæ‚¨ä½¿ç”¨çš„æ˜¯ 'API Token' è€Œä¸æ˜¯ 'ç™»å…¥å¯†ç¢¼'ã€‚")
        sys.exit(1)

def create_new_report(latest_page, dates):
    """åŸºæ–¼èˆŠå…§å®¹å»ºç«‹æ–°é é¢"""
    new_title = f"WeeklyReport_{dates['filename']}"
    print(f"æº–å‚™å»ºç«‹æ–°é é¢: {new_title}")
    
    original_body = latest_page['body']['storage']['value']
    
    # æ—¥æœŸæ›¿æ›é‚è¼¯
    found_dates = re.findall(r"\d{4}-\d{1,2}-\d{1,2}", original_body)
    new_body = original_body
    
    if len(found_dates) >= 2:
        old_start = found_dates[0]
        old_end = found_dates[1]
        print(f"åµæ¸¬åˆ°èˆŠæ—¥æœŸå€é–“: {old_start} ~ {old_end}")
        new_body = new_body.replace(old_start, dates['monday_str'], 1)
        new_body = new_body.replace(old_end, dates['sunday_str'], 1)
        print(f"å·²æ›¿æ›ç‚º: {dates['monday_str']} ~ {dates['sunday_str']}")
    else:
        print("âš ï¸ èˆŠå…§å®¹ä¸­æ‰¾ä¸åˆ°æ—¥æœŸæ ¼å¼ï¼Œç›´æ¥è¤‡è£½å…§å®¹ã€‚")

    # æº–å‚™ Payload
    ancestors = []
    if latest_page.get('ancestors'):
        ancestors.append({'id': latest_page['ancestors'][-1]['id']})
    
    space_key = latest_page['space']['key']
    
    payload = {
        "title": new_title,
        "type": "page",
        "space": {"key": space_key},
        "ancestors": ancestors,
        "body": {
            "storage": {
                "value": new_body,
                "representation": "storage"
            }
        }
    }
    
    headers = {"Content-Type": "application/json"}
    
    try:
        response = requests.post(
            API_ENDPOINT, 
            auth=HTTPBasicAuth(USERNAME, API_TOKEN),
            headers=headers,
            data=json.dumps(payload)
        )
        response.raise_for_status()
        new_page_data = response.json()
        
        # çµ„åˆ WebUI é€£çµ
        webui = new_page_data['_links']['webui']
        full_link = f"{BASE_URL}/wiki{webui}"
        
        print(f"ğŸ‰ æˆåŠŸå»ºç«‹é é¢ï¼")
        print(f"é é¢ ID: {new_page_data['id']}")
        print(f"é€£çµ: {full_link}")
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        print(f"ä¼ºæœå™¨å›æ‡‰: {response.text}")
        if "title already exists" in response.text:
            print("ğŸ’¡ åŸå› : è©²æ¨™é¡Œçš„é€±å ±å·²ç¶“å­˜åœ¨äº†ï¼")

def main():
    dates = get_target_dates()
    print(f"=== Confluence API è‡ªå‹•é€±å ±è…³æœ¬ (v3.0 å¼·åŠ›æ·¨åŒ–ç‰ˆ) ===")
    print(f"ç›®æ¨™æ—¥æœŸ: {dates['monday_str']} ~ {dates['sunday_str']}")
    
    try:
        latest_page = find_latest_report()
        create_new_report(latest_page, dates)
    except Exception as e:
        print(f"åŸ·è¡Œä¸­æ–·: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
