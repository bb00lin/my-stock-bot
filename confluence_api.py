import os
import requests
import json
import re
import sys
from datetime import datetime, timedelta
from requests.auth import HTTPBasicAuth
from urllib.parse import urlparse

# --- è¨­å®šå€ ---
RAW_URL = os.environ.get("CONF_URL")
USERNAME = os.environ.get("CONF_USER")
API_TOKEN = os.environ.get("CONF_PASS")

if not RAW_URL or not USERNAME or not API_TOKEN:
    print("éŒ¯èª¤ï¼šç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

parsed = urlparse(RAW_URL)
BASE_URL = f"{parsed.scheme}://{parsed.netloc}"
API_ENDPOINT = f"{BASE_URL}/wiki/rest/api/content"

def find_latest_report():
    print("æ­£åœ¨æœå°‹æœ€æ–°é€±å ±...")
    # æœå°‹æ¨™é¡ŒåŒ…å« "WeeklyReport" çš„é é¢ï¼ŒæŒ‰å»ºç«‹æ™‚é–“å€’åº
    cql = 'type=page AND title ~ "WeeklyReport*" ORDER BY created DESC'
    
    url = f"{API_ENDPOINT}/search"
    params = {'cql': cql, 'limit': 1, 'expand': 'body.storage,ancestors,space'}
    
    try:
        response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=params)
        response.raise_for_status()
        results = response.json().get('results', [])
        
        if not results:
            print("âš ï¸ æ‰¾ä¸åˆ°ä»»ä½•åŸºæº–é€±å ±ï¼Œç„¡æ³•æ¨ç®—ä¸‹ä¸€æœŸã€‚")
            sys.exit(1)
        
        latest = results[0]
        print(f"âœ… æ‰¾åˆ°åŸºæº–é€±å ±: {latest['title']} (ID: {latest['id']})")
        return latest
    except Exception as e:
        print(f"âŒ æœå°‹å¤±æ•—: {e}")
        sys.exit(1)

def calculate_next_date(latest_title):
    """
    å¾æœ€æ–°é€±å ±çš„æ¨™é¡Œè§£ææ—¥æœŸï¼Œä¸¦æ¨ç®—ä¸‹é€±äº”
    ä¾‹å¦‚: WeeklyReport_20260123 -> ä¸‹ä¸€æœŸ 20260130
    """
    # å˜—è©¦å¾æ¨™é¡ŒæŠ“å– 8 ç¢¼æ•¸å­—
    match = re.search(r"(\d{8})", latest_title)
    if match:
        last_date_str = match.group(1)
        try:
            last_date = datetime.strptime(last_date_str, "%Y%m%d").date()
            
            # é‚è¼¯ï¼šä¸‹ä¸€æœŸ = åŸºæº–æ—¥ + 7å¤©
            next_date = last_date + timedelta(days=7)
            
            # è¨ˆç®—è©²é€±çš„é€±ä¸€èˆ‡é€±æ—¥ (ç”¨æ–¼ JQL æ›¿æ›)
            # next_date æ˜¯é€±äº”
            monday = next_date - timedelta(days=4)
            sunday = next_date + timedelta(days=2)
            
            return {
                "filename": next_date.strftime("%Y%m%d"),
                "monday_str": monday.strftime("%Y-%m-%d"),
                "sunday_str": sunday.strftime("%Y-%m-%d")
            }
        except ValueError:
            pass
            
    # å¦‚æœæ¨™é¡Œç„¡æ³•è§£æï¼Œå°±é€€å›ä½¿ç”¨ã€Œæœ¬é€±äº”ã€
    print("âš ï¸ ç„¡æ³•å¾æ¨™é¡Œè§£ææ—¥æœŸï¼Œå°‡ä½¿ç”¨æœ¬é€±æ—¥æœŸä½œç‚ºåŸºæº–ã€‚")
    today = datetime.now().date()
    monday = today - timedelta(days=today.weekday())
    sunday = monday + timedelta(days=6)
    friday = monday + timedelta(days=4)
    return {
        "filename": friday.strftime("%Y%m%d"),
        "monday_str": monday.strftime("%Y-%m-%d"),
        "sunday_str": sunday.strftime("%Y-%m-%d")
    }

def create_new_report(latest_page):
    # 1. è¨ˆç®—ä¸‹ä¸€æœŸæ—¥æœŸ
    next_dates = calculate_next_date(latest_page['title'])
    new_title = f"WeeklyReport_{next_dates['filename']}"
    print(f"æº–å‚™å»ºç«‹ä¸‹ä¸€æœŸé€±å ±: {new_title}")
    print(f"æ–°é€±æœŸå€é–“: {next_dates['monday_str']} ~ {next_dates['sunday_str']}")
    
    # 2. æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ (é›™é‡ç¢ºèª)
    check_url = f"{API_ENDPOINT}/search"
    check_params = {'cql': f'title = "{new_title}"'}
    check_resp = requests.get(check_url, auth=HTTPBasicAuth(USERNAME, API_TOKEN), params=check_params)
    if check_resp.json().get('results'):
        print(f"âš ï¸ è·³éï¼šé é¢ '{new_title}' å·²ç¶“å­˜åœ¨ï¼")
        return

    # 3. è™•ç†å…§å®¹
    original_body = latest_page['body']['storage']['value']
    new_body = original_body
    
    found_dates = re.findall(r"\d{4}-\d{1,2}-\d{1,2}", original_body)
    if len(found_dates) >= 2:
        old_start, old_end = found_dates[0], found_dates[1]
        print(f"æ›¿æ› JQL æ—¥æœŸ: {old_start} -> {next_dates['monday_str']}")
        
        new_body = new_body.replace(old_start, next_dates['monday_str'], 1)
        new_body = new_body.replace(old_end, next_dates['sunday_str'], 1)
    
    # 4. å»ºç«‹é é¢
    ancestors = []
    if latest_page.get('ancestors'):
        ancestors.append({'id': latest_page['ancestors'][-1]['id']})
    
    payload = {
        "title": new_title,
        "type": "page",
        "space": {"key": latest_page['space']['key']},
        "ancestors": ancestors,
        "body": {
            "storage": {
                "value": new_body,
                "representation": "storage"
            }
        }
    }
    
    try:
        response = requests.post(
            API_ENDPOINT, 
            auth=HTTPBasicAuth(USERNAME, API_TOKEN),
            headers={"Content-Type": "application/json"},
            data=json.dumps(payload)
        )
        response.raise_for_status()
        data = response.json()
        webui = data['_links']['webui']
        link = f"{BASE_URL}/wiki{webui}" if not webui.startswith('/wiki') else f"{BASE_URL}{webui}"
        
        print(f"ğŸ‰ æˆåŠŸå»ºç«‹ï¼é€£çµ: {link}")
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        print(response.text)

def main():
    print(f"=== Confluence API è‡ªå‹•é€±å ± (v5.0 æ™ºæ…§éå¢ç‰ˆ) ===")
    latest_page = find_latest_report()
    create_new_report(latest_page)

if __name__ == "__main__":
    main()
